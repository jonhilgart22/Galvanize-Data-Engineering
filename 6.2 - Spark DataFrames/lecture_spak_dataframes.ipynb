{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"images/spark-logo.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Where are we?\n",
    "----\n",
    "\n",
    "![](http://www.platfora.com/wp-content/uploads/2015/06/gartner-hype-cycle.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark is (relatively) platform agnostic. \n",
    "\n",
    "The goal is:\n",
    "> Write once, run every where"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Describe Spark's architecture and execution model\n",
    "- Persist and cache data\n",
    "- Create word count in Spark\n",
    "- Understand and use Spark DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "Spark Architecture Review\n",
    "---\n",
    "\n",
    "![](images/spark-cluster.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/plan.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/save_time.png\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "RDD Laziness\n",
    "------------\n",
    "\n",
    "![](https://prateekvjoshi.files.wordpress.com/2014/10/3-laziness.png)\n",
    "\n",
    "Q: What is this Spark job doing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.2 ms, sys: 22.5 ms, total: 39.8 ms\n",
      "Wall time: 7.94 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 10000000\n",
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How is the following job different from the previous one? How long do you expect it to take?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.27 ms, sys: 1.93 ms, total: 4.2 ms\n",
      "Wall time: 6.35 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PythonRDD[3] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time sc.parallelize(xrange(max)).map(lambda x:x+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Why did the second job complete so much faster?\n",
    "</summary>\n",
    "1. Because Spark is lazy. \n",
    "<br>\n",
    "2. Transformations produce new RDDs and do no operations on the data.\n",
    "<br>\n",
    "3. Nothing happens until an action is applied to an RDD.\n",
    "<br>\n",
    "4. An RDD is the *recipe* for a transformation, rather than the\n",
    "   *result* of the transformation.\n",
    "</details>\n",
    "<br>\n",
    "<br>\n",
    "<details><summary>\n",
    "What is the benefit of keeping the recipe instead of the result of the action?\n",
    "</summary>\n",
    "1. It save memory.\n",
    "<br>\n",
    "2. It produces *resilience*. \n",
    "<br>\n",
    "3. If an RDD loses data on a machine, it always knows how to recompute it.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Deep Dive on DAGs\n",
    "---\n",
    "\n",
    "![](http://image.slidesharecdn.com/2014-05-29-spark-140529034646-phpapp01/95/spark-internals-hadoop-source-code-reading-16-in-japan-10-638.jpg?cb=1401392040)\n",
    "\n",
    "![](http://image.slidesharecdn.com/sparkoverview-140729190732-phpapp01/95/spark-overview-18-638.jpg?cb=1406660911)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Caching and Persistence\n",
    "-----\n",
    "\n",
    "Consider this Spark job:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "num_count = 500*1000\n",
    "num_list = [random.random() for i in xrange(num_count)]\n",
    "rdd1 = sc.parallelize(num_list)\n",
    "rdd2 = rdd1.sortBy(lambda num: num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets time running `count()` on `rdd2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.36 ms, sys: 4.91 ms, total: 14.3 ms\n",
      "Wall time: 2.26 s\n",
      "CPU times: user 6.98 ms, sys: 1.84 ms, total: 8.82 ms\n",
      "Wall time: 849 ms\n",
      "CPU times: user 6.22 ms, sys: 1.66 ms, total: 7.88 ms\n",
      "Wall time: 954 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RDD does no work until an action is called. And then when an action is called it figures out the answer and then throws away all the data.\n",
    "\n",
    "If you have an RDD that you are going to reuse in your computation you can use `cache()` to make Spark cache the RDD.\n",
    "\n",
    "Lets cache it and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.3 ms, sys: 3.04 ms, total: 8.34 ms\n",
      "Wall time: 1.29 s\n",
      "CPU times: user 4.99 ms, sys: 1.58 ms, total: 6.57 ms\n",
      "Wall time: 168 ms\n",
      "CPU times: user 7.14 ms, sys: 1.94 ms, total: 9.08 ms\n",
      "Wall time: 377 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.cache()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()\n",
    "%time rdd2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching the RDD speeds up the job because the RDD does not have to be computed from scratch again.\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- Calling `cache()` flips a flag on the RDD. \n",
    "\n",
    "- The data is not cached until an action is called.\n",
    "\n",
    "- You can uncache an RDD using `unpersist()`.\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will `unpersist` uncache the RDD immediately or does it wait for an\n",
    "action?\n",
    "</summary>\n",
    "It unpersists immediately.\n",
    "</details>\n",
    "\n",
    "Caching and Persistence\n",
    "-----------------------\n",
    "\n",
    "Q: Persist RDD to disk instead of caching it in memory.\n",
    "\n",
    "- You can cache RDDs at different levels.\n",
    "\n",
    "- Here is an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[20] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "rdd = sc.parallelize(xrange(100))\n",
    "rdd.persist(pyspark.StorageLevel.DISK_ONLY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Will the RDD be stored on disk at this point?\n",
    "</summary>\n",
    "No. It will get stored after we call an action.\n",
    "</details>\n",
    "\n",
    "Persistence Levels\n",
    "------------------\n",
    "\n",
    "Level                      |Meaning\n",
    "-----                      |-------\n",
    "`MEMORY_ONLY`              |Same as `cache()`\n",
    "`MEMORY_AND_DISK`          |Cache in memory then overflow to disk\n",
    "`MEMORY_AND_DISK_SER`      |Like above; in cache keep objects serialized instead of live \n",
    "`DISK_ONLY`                |Save to disk, not to memory\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `MEMORY_AND_DISK_SER` is a good compromise between the levels. \n",
    "\n",
    "- Fast, but not too expensive.\n",
    "\n",
    "- Make sure you unpersist when you don't need the RDD any more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Word Count\n",
    "---\n",
    "\n",
    "The \"Hello, world!\" of Big Data.\n",
    "\n",
    "Know it, üòç it\n",
    "\n",
    "1) Create some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing quotes.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile quotes.txt\n",
    "You‚Äôre fired\n",
    "You‚Äôre fired\n",
    "I will build a great wall and nobody builds walls better than me, believe me and I‚Äôll build them very inexpensively. \n",
    "You‚Äôre fired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Count the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'and', 2),\n",
       " (u'', 1),\n",
       " (u'them', 1),\n",
       " (u'I', 1),\n",
       " (u'very', 1),\n",
       " (u'will', 1),\n",
       " (u'great', 1),\n",
       " (u'better', 1),\n",
       " (u'me', 1),\n",
       " (u'walls', 1),\n",
       " (u'a', 1),\n",
       " (u'I\\u2019ll', 1),\n",
       " (u'wall', 1),\n",
       " (u'fired', 3),\n",
       " (u'me,', 1),\n",
       " (u'nobody', 1),\n",
       " (u'You\\u2019re', 3),\n",
       " (u'build', 2),\n",
       " (u'inexpensively.', 1),\n",
       " (u'believe', 1),\n",
       " (u'than', 1),\n",
       " (u'builds', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sc.textFile('quotes.txt')\n",
    "     .flatMap(lambda line: line.split(\" \"))\n",
    "     .map(lambda word: (word,1))\n",
    "     .reduceByKey(lambda count1, count2: count1+count2)\n",
    "     .collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Handling tabular data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sales_by_state = (sc.textFile('sales.csv')\n",
    "                  .map(lambda x: x.split(\",\"))\n",
    "                  .filter(lambda x: not x[0].startswith('#'))\n",
    "                  .map(lambda x: (x[-3],float(x[-1])))\n",
    "                  .reduceByKey(lambda amount1,amount2: amount1+amount2)\n",
    "                  .sortBy(lambda state_amount:state_amount[1],ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales_by_state.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this code looks reasonable, the list indexes are cryptic and hard to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "DataFrames, you might remeber them from Pandas üêº\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just automagically load a csv.\n",
    "\n",
    "[Source](https://community.cloud.databricks.com/?o=6058142077065523#externalnotebook/https%3A%2F%2Fdocs.cloud.databricks.com%2Fdocs%2Flatest%2Fdatabricks_guide%2Findex.html%2303%2520Accessing%2520Data%2F3%2520Common%2520File%2520Formats%2F1%2520CSV%2520-%2520py.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Read csv data as DataFrame, Scala-style\n",
    "# sales = (sqlContext.read.format('com.databricks.spark.csv')\n",
    "#             .options(header='true', inferSchema='true')\n",
    "#             .load('/FileStore/tables/3onpii8c1465685311162/sales.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Read csv data as DataFrame, Python-style\n",
    "# sales = spark.read.csv('/FileStore/tables/aef5f0rv1465685940318/sales.csv',\n",
    "#                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sales.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sales_by_state = (sales\n",
    "#                   .groupBy(\"state\")\n",
    "#                   .sum(\"amount\")\n",
    "#                   .withColumnRenamed(\"sum(amount)\", \"total_sales\")\n",
    "#                   .orderBy(\"total_sales\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# display(sales_by_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Why Spark Sparkles ‚ú®\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame API\n",
    "\n",
    "> A DataFrame is a distributed collection of data organized into __named__ columns. \n",
    "\n",
    "> It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/dataframes_in_spark_stack.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames and Spark SQL. These are high-level APIs for working with structured data (e.g. database tables, JSON files), which let Spark automatically optimize both storage and computation. DataFrame API that provides a type-safe, object-oriented programming interface. \n",
    "\n",
    "DataFrames can be constructed from a wide array of sources such as: structured data files, tables in Hive, external databases, or existing RDDs. \n",
    "\n",
    "Behind these APIs, the Catalyst optimizer and Tungsten execution engine optimize applications in ways that were not possible with Spark‚Äôs object-oriented (RDD) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Where is the DataFrames API\n",
    "----\n",
    "\n",
    "`pyspark.sql.SQLContext`\n",
    "\n",
    "Main entry point for DataFrame and SQL functionality.\n",
    "\n",
    "[RTFM](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.sql.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark 2.0 has simplier API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark DataSet API\n",
    "----\n",
    "\n",
    "A DataSet is a new interface that tries to provide the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL‚Äôs optimized execution engine. \n",
    "\n",
    "A DataSet can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). \n",
    "\n",
    "Spark DataSet are strongly-typed, immutable collection of objects that are mapped to a relational schema with object-oriented programming interface. \n",
    "\n",
    "[Source 1](https://docs.cloud.databricks.com/docs/latest/databricks_guide/05%20Spark/1%20Intro%20Datasets.html)  \n",
    "[Source 2](https://databricks.com/blog/2016/01/04/introducing-apache-spark-datasets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSet are \"closer to the metal\" / JVM thus allows for increased optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"https://www.stayathomemum.com.au/wp-content/uploads/2013/10/tantrum.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Only available in Scala and Java. Not in Python.\n",
    "\n",
    "[DataSet documentation](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/sql-programming-guide.html#creating-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Spark is the üí©: can do Word Count simply and has features for advanced operations.\n",
    "- There are tricks to optimally Persist and Cache data.\n",
    "- Spark DataFrames üíó ~ Big Data Pandas\n",
    "    - Use them as much as possible\n",
    "    - More _Data Science_ features\n",
    "- Keep an eye out for DataSets (or learn Scala)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
