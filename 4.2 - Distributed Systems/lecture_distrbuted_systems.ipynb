{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Distributed Computing\n",
    "---\n",
    "\n",
    "![](images/cloud-cartoon.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is distributed computing?\n",
    "----\n",
    "\n",
    "__Many computers with shared computing goal__:\n",
    "![](images/clientserver.png)\n",
    "\n",
    "[Additional Slides](http://wla.berkeley.edu/~cs61a/fa11/61a-python/content/www/slides/61A-031-Distributed_1pps.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Why distrubted computing?\n",
    "----\n",
    "\n",
    "Massive gains in performance and effiency. Horizontal scaling!\n",
    "\n",
    "__Downsides__:\n",
    "- Coordination overhead\n",
    "- Harder to reason about and debug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Characteristics of distributed systems\n",
    "---\n",
    "1. Independent computers\n",
    "2. (Often) In different locations\n",
    "3. Connected by a network\n",
    "4. Communicate by passing messages to each other\n",
    "5. A shared computational goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/architecture.png)\n",
    "\n",
    "Other architectures are possible (e.g., peer-to-peer)\n",
    "\n",
    "SPOF = __Single Point of Failure__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Why is distrubted computing hard?\n",
    "---\n",
    "\n",
    "Come on, I just put a bunch computers together on a local network. \n",
    "\n",
    "__Fallacies of distributed computing:__\n",
    "\n",
    "- The network is reliable.\n",
    "- Latency is zero.\n",
    "- Bandwidth is infinite.\n",
    "- The network is secure.\n",
    "- Topology doesn’t change.\n",
    "- There is one administrator.\n",
    "- Transport cost is zero.\n",
    "- The network is homogeneous.\n",
    "\n",
    "[Source](https://en.wikipedia.org/wiki/Fallacies_of_distributed_computing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/messages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means there will be __Logs, Logs, Logs__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "How Data Scientists can use Distributed Computing?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Motivating Story\n",
    "----\n",
    "\n",
    "![](images/cluster_speedup.png)\n",
    "\n",
    "When doing model evaluations and parameters tuning, many models must be trained independently on the same data. This is an embarrassingly parallel problem but having a copy of the dataset in memory for each process is waste of RAM:\n",
    "\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/grid_search_parameters.png\" style=\"display:inline; width: 49%\" />\n",
    "<img src=\"https://s3-us-west-2.amazonaws.com/dsci6007/assets/grid_search_cv_splits.png\" style=\"display:inline; width: 49%\" />\n",
    "\n",
    "### Parallel Model Selection and Assessment\n",
    "When doing 3 folds cross validation on a 9 parameters grid, a naive implementation could read the data from the disk and load it in memory 27 times. If this happens concurrently (e.g. on a compute node with 32 cores) the RAM might blow up hence breaking the potential linear speed up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for understanding\n",
    "<details><summary>How many copies would be needed to do 5-fold cross-validation if we added `1000` as a possibility for `param_1`?</summary>\n",
    "60</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for understanding\n",
    "\n",
    "<details><summary>What does it mean for a task to be \"embarrassingly parallel\"?</summary>\n",
    "Little or no effort is required to separate the problem into a number of parallel tasks.</details>\n",
    "\n",
    "<details><summary>List some algorithms that are and are not embarrassingly parallel</summary>\n",
    "<table><tr><th>Embarassingly Parallel<th>Not Embarassingly Parallel</th></tr>\n",
    "<tr><td>Random Forests</td><td>AdaBoost</td></tr>\n",
    "<tr><td>Grid Search</td><td>MaxEnt</td></tr>\n",
    "<tr><td>...</td><td>...</td></tr>\n",
    "</table></details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How Jupyter works\n",
    "----\n",
    "\n",
    "![](http://ipython.readthedocs.org/en/stable/_images/ipy_kernel_and_terminal.png)\n",
    "\n",
    "It is a Unix Big Data System!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`ipyparallel`, a Primer\n",
    "---\n",
    "\n",
    "*Adapted from [Olivier Grisel](http://ogrisel.com/)'s [Parallel Machine Learning Tutorial](https://github.com/ogrisel/parallel_ml_tutorial) on [Distributed Model Selection and Assessment](https://github.com/ogrisel/parallel_ml_tutorial/blob/master/notebooks/06%20-%20Distributed%20Model%20Selection%20and%20Assessment.ipynb)*\n",
    "\n",
    "This section gives a primer on some tools best utilizing computational resources when doing predictive modeling in the Python / NumPy ecosystem namely:\n",
    "\n",
    "- optimal usage of available CPUs and cluster nodes with **`ipyparallel`**\n",
    "\n",
    "- optimal memory re-use using shared memory between Python processes using **`numpy.memmap`** and **`joblib`**\n",
    "\n",
    "### What is so great about `ipyparallel`:\n",
    "\n",
    "- Single node multi-CPUs\n",
    "- Multiple node multi-CPUs\n",
    "- Interactive In-memory computing\n",
    "- Jupyter integration with `%px` and `%%px` magics\n",
    "- Possibility to interactively connect to individual computing processes to launch interactive debugger (`#priceless`)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Installing IPython for parallel computing\n",
    "---\n",
    "\n",
    "[repo](https://github.com/ipython/ipyparallel)\n",
    "\n",
    "[Read the docs](http://ipyparallel.readthedocs.org/en/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Architecture overview\n",
    "---\n",
    "\n",
    "![](http://ipyparallel.readthedocs.org/en/latest/_images/wideView.png)\n",
    "\n",
    "[Source](http://ipyparallel.readthedocs.org/en/latest/intro.html#architecture-overview)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get started:\n",
    "\n",
    "Let start an IPython cluster using the `ipcluster` common (usually run from your operating system console). To make sure that we are not running several clusters on the same host, let's try to shut down any running IPython cluster first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-06-06 09:31:36.212 [IPClusterStop] CRITICAL | Could not read pid file, cluster is probably not running.\r\n"
     ]
    }
   ],
   "source": [
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ipcluster start -n=4 --daemon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Go to the \"Cluster\" tab of the notebook and **start a local cluster with 2 engines**. Then come back here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now be able to use our cluster from our notebook session (or any other Python process running on localhost):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ipyparallel import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "len(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The %px and %%px magics\n",
    "\n",
    "All the engines of the client can be accessed imperatively using the `%px` and `%%px` IPython cell magics:\n",
    "\n",
    "[Docs](http://ipyparallel.readthedocs.io/en/latest/magics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] This is running in process with pid 77599 on host 'Alessandros-MacBook-Pro.local'.\n",
      "[stdout:1] This is running in process with pid 77601 on host 'Alessandros-MacBook-Pro.local'.\n",
      "[stdout:2] This is running in process with pid 77600 on host 'Alessandros-MacBook-Pro.local'.\n",
      "[stdout:3] This is running in process with pid 77602 on host 'Alessandros-MacBook-Pro.local'.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "import os\n",
    "import socket\n",
    "\n",
    "print(\"This is running in process with pid {0} on host '{1}'.\".format(\n",
    "      os.getpid(), socket.gethostname()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content of the `__main__` namespace can also be read and written via the `%px` magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px a = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 1\n",
      "[stdout:1] 1\n",
      "[stdout:2] 1\n",
      "[stdout:3] 1\n"
     ]
    }
   ],
   "source": [
    "%px print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 2\n",
      "[stdout:1] 2\n",
      "[stdout:2] 2\n",
      "[stdout:3] 2\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "a *= 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to restrict the `%px` and `%%px` magic instructions to specific engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "%%px --targets=-1\n",
    "a *= 2\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 2\n",
      "[stdout:1] 2\n",
      "[stdout:2] 2\n",
      "[stdout:3] 4\n"
     ]
    }
   ],
   "source": [
    "%px print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The DirectView objects\n",
    "\n",
    "Cell magics are very nice to work interactively from the notebook but it's also possible to replicate their behavior programmatically with more flexibility with a `DirectView` instance. A `DirectView` can be created by slicing the client object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [0, 1, 2, 3]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_engines = client[:]\n",
    "all_engines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The namespace of the `__main__` module of each running python engine can be accessed in read and write mode as a python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_engines['ab'] = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[13, 13, 13, 13]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_engines['ab']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] 13\n",
      "[stdout:1] 13\n",
      "[stdout:2] 13\n",
      "[stdout:3] 13\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct views can also execute the same code in parallel on each engine of the view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: my_sum>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_sum(a, b):\n",
    "    return a + b\n",
    "\n",
    "my_sum_apply_results = all_engines.apply(my_sum, 11, 31)\n",
    "my_sum_apply_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput of the `apply` method is an asynchronous handle returned immediately without waiting for the end of the computation. To block until the results are ready use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[42, 42, 42, 42]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sum_apply_results.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a more useful example to fetch the network hostname of each engine in the cluster. Let's study it in more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hostname():\n",
    "    \"\"\"Return the name of the host where the function is being called\"\"\"\n",
    "    import socket\n",
    "    return socket.gethostname()\n",
    "\n",
    "hostname_apply_result = all_engines.apply(hostname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the above, the `hostname` function is first defined locally (the client python process). The `DirectView.apply` method introspects it, serializes its name and bytecode and ships it to each engine of the cluster where it is reconstructed as local function on each engine. This function is then called on each engine of the view with the optionally provided arguments.\n",
    "\n",
    "In return, the client gets a python object that serves as an handle to asynchronously fetch the list of the results of the calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: hostname:finished>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname_apply_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Alessandros-MacBook-Pro.local',\n",
       " 'Alessandros-MacBook-Pro.local',\n",
       " 'Alessandros-MacBook-Pro.local',\n",
       " 'Alessandros-MacBook-Pro.local']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostname_apply_result.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to key the results explicitly with the engine ids with the `AsyncResult.get_dict` method. This is a very simple idiom to fetch metadata on the runtime environment of each engine of the direct view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Alessandros-MacBook-Pro.local',\n",
       " 1: 'Alessandros-MacBook-Pro.local',\n",
       " 2: 'Alessandros-MacBook-Pro.local',\n",
       " 3: 'Alessandros-MacBook-Pro.local'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hostnames = hostname_apply_result.get_dict()\n",
    "hostnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be handy to invert this mapping to find one engine id per host in the cluster so as to execute host specific operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 3}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_engine_by_host = dict((hostname, engine_id) for engine_id, hostname\n",
    "                      in hostnames.items())\n",
    "one_engine_by_host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DirectView [3]>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_engine_per_host_view = client[one_engine_by_host.values()]\n",
    "one_engine_per_host_view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trick:** you can even use those engines ids to execute shell commands in parallel on each host of the cluster:\n",
    "\n",
    "```python\n",
    "%%px --targets=one_engine_by_host.values()\n",
    "\n",
    "!pip install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_engine_by_host.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note on Importing Modules on Remote Engines\n",
    "\n",
    "In the previous example we put the `import socket` statement inside the body of the `hostname` function to make sure to make sure that is is available when the rest of the function is executed in the python processes of the remote engines.\n",
    "\n",
    "Alternatively it is possible to import the required modules ahead of time on all the engines of a directview using a context manager / with syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing numpy on engine(s)\n"
     ]
    }
   ],
   "source": [
    "with all_engines.sync_imports():\n",
    "    import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However this method does **not** support alternative import syntaxes:\n",
    "    \n",
    "    >>> import numpy as np\n",
    "    >>> from numpy import linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence the method of importing in the body of the \"applied\" functions is more flexible. Additionally, this does not pollute the `__main__` namespace of the engines as it only impact the local namespace of the function itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Optional Exercise**:\n",
    "---\n",
    "\n",
    "- Write a function that returns the amemory usage of each engine process in the cluster.\n",
    "- Allocate a largish numpy array of zeros of known size (e.g. 100MB) on each engine of the cluster.\n",
    "\n",
    "Hints:\n",
    "\n",
    "Use the `psutil` module to collect the runtime info on a specific process or host. For instance to fetch the memory usage of the currently running process in MB:\n",
    "\n",
    "    >>> import os\n",
    "    >>> import psutil\n",
    "    >>> psutil.Process(os.getpid()).get_memory_info().rss / 1e6\n",
    "\n",
    "To allocate a numpy array with 1000 zeros stored as 64bit floats you can use:\n",
    "\n",
    "    >>> import numpy as np\n",
    "    >>> z = np.zeros(1000, dtype=np.float64)\n",
    "\n",
    "The size in bytes of such a numpy array can then be fetched with ``z.nbytes``:\n",
    "    \n",
    "    >>> z.nbytes / 1e6\n",
    "    0.008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Balanced View"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LoadBalancedView` is an alternative to the `DirectView` to run one function call at a time on a free engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lv = client.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def slow_square(x):\n",
    "    import time\n",
    "    time.sleep(2)\n",
    "    return x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = lv.apply(slow_square, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncResult: slow_square>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.get()  # blocking call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to spread some tasks among the engines of the LB view by passing a callable and an iterable of task arguments to the `LoadBalancedView.map` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AsyncMapResult: slow_square>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = lv.map(slow_square, [0, 1, 2, 3])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# results.abort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "# Iteration on AsyncMapResult is blocking\n",
    "for r in results:\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The load balanced view will be used in the following to schedule work on the cluster while being able to monitor progress and occasionally add new computing nodes to the cluster while computing to speed up the processing when using EC2 and StarCluster (see later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pop Quiz\n",
    "<details><summary>What is the difference between a direct view and a load balanced view?</summary>\n",
    "In a direct view, engines are addressed explicitly.<br />\n",
    "In a load balanced view, the scheduler is trusted with assigning work to appropriate engines.</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharing Read-only Data between Processes on the Same Host with Memmapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numpy package makes it possible to memory map large contiguous chunks of binary files as shared memory for all the Python processes running on a given host:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%px import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a `numpy.memmap` instance with the `w+` mode creates a file on the filesystem and zeros its content. Let's do it from the first engine process or our current IPython cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "# Cleanup any existing file from past session (necessary for windows)\n",
    "import os\n",
    "if os.path.exists('small.mmap'):\n",
    "    os.unlink('small.mmap')\n",
    "\n",
    "mm_w = np.memmap('small.mmap', shape=10, dtype=np.float32, mode='w+')\n",
    "print(mm_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This binary file can then be mapped as a new numpy array by all the engines having access to the same filesystem. The `mode='r+'` opens this shared memory area in read write mode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[stdout:1] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[stdout:2] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[stdout:3] [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "\n",
    "mm_r = np.memmap('small.mmap', dtype=np.float32, mode='r+')\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "mm_w[0] = 42\n",
    "print(mm_w)\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] [ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:1] [ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:2] [ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:3] [ 42.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "%px print(mm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory mapped arrays created with `mode='r+'` can be modified and the modifications are shared with all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%px --targets=1\n",
    "\n",
    "mm_r[1] = 43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] [ 42.  43.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:1] [ 42.  43.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:2] [ 42.  43.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "[stdout:3] [ 42.  43.   0.   0.   0.   0.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(mm_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful those, there is no builtin read nor write lock available on this such datastructures so it's better to avoid concurrent read & write operations on the same array segments unless there engine operations are made to cooperate with some synchronization or scheduling orchestrator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memmap arrays generally behave very much like regular in-memory numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] sum=85., mean=8.5, std=17.\n",
      "[stdout:1] sum=85., mean=8.5, std=17.\n",
      "[stdout:2] sum=85., mean=8.5, std=17.\n",
      "[stdout:3] sum=85., mean=8.5, std=17.\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "print(\"sum={0:.3}, mean={1:.3}, std={2:.3}\".format(\n",
    "    mm_r.sum(), np.mean(mm_r), np.std(mm_r)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before allocating more data in memory on the cluster let us define a couple of utility functions from the previous exercise (and more) to monitor what is used by which engine and what is still free on the cluster as a whole:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_host_free_memory(client):\n",
    "    \"\"\"Free memory on each host of the cluster in MB.\"\"\"\n",
    "    all_engines = client[:]\n",
    "    def hostname():\n",
    "        import socket\n",
    "        return socket.gethostname()\n",
    "    \n",
    "    hostnames = all_engines.apply(hostname).get_dict()\n",
    "    one_engine_per_host = dict((hostname, engine_id)\n",
    "                               for engine_id, hostname\n",
    "                               in hostnames.items())\n",
    "\n",
    "    def host_free_memory():\n",
    "        import psutil\n",
    "        return psutil.virtual_memory().free / 1e6\n",
    "    \n",
    "    \n",
    "    host_mem = client[one_engine_per_host.values()].apply(\n",
    "        host_free_memory).get_dict()\n",
    "    \n",
    "    return dict((hostnames[eid], m) for eid, m in host_mem.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 1522.712576}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's allocate a 80MB memmap array in the first engine and load it in readwrite mode in all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:14]: \u001b[0mmemmap([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "# Cleanup any existing file from past session (necessary for windows)\n",
    "import os\n",
    "if os.path.exists('big.mmap'):\n",
    "    os.unlink('big.mmap')\n",
    "\n",
    "np.memmap('big.mmap', shape=10 * int(1e6), dtype=np.float64, mode='w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] -rw-r--r--  1 alessandro  staff    76M Jun  6 09:39 big.mmap\r\n",
      "[stdout:1] -rw-r--r--  1 alessandro  staff    76M Jun  6 09:39 big.mmap\r\n",
      "[stdout:2] -rw-r--r--  1 alessandro  staff    76M Jun  6 09:39 big.mmap\r\n",
      "[stdout:3] -rw-r--r--  1 alessandro  staff    76M Jun  6 09:39 big.mmap\r\n"
     ]
    }
   ],
   "source": [
    "%%px\n",
    "ls -lh big.mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 1522.93376}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant memory was used in this operation as we just asked the OS to allocate the buffer on the hard drive and just maitain a virtual memory area as a cheap reference to this buffer.\n",
    "\n",
    "Let's open new references to the same buffer from all the engines at once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "CPU times: user 249 µs, sys: 750 µs, total: 999 µs\n",
      "Wall time: 10.8 ms\n",
      "[stdout:1] \n",
      "CPU times: user 217 µs, sys: 318 µs, total: 535 µs\n",
      "Wall time: 25.5 ms\n",
      "[stdout:2] \n",
      "CPU times: user 513 µs, sys: 1.04 ms, total: 1.55 ms\n",
      "Wall time: 24.3 ms\n",
      "[stdout:3] \n",
      "CPU times: user 278 µs, sys: 1.3 ms, total: 1.58 ms\n",
      "Wall time: 19.2 ms\n"
     ]
    }
   ],
   "source": [
    "%px %time big_mmap = np.memmap('big.mmap', dtype=np.float64, mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:13]: \u001b[0mmemmap([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:14]: \u001b[0mmemmap([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:13]: \u001b[0mmemmap([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:17]: \u001b[0mmemmap([ 0.,  0.,  0., ...,  0.,  0.,  0.])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%px big_mmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 1520.328704}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No physical memory was allocated in the operation as it just took a couple of ms to do so. This is also confirmed by the engines process stats:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's trigger an actual load of the data from the drive into the in-memory disk cache of the OS, this can take some time depending on the speed of the hard drive (on the order of 100MB/s to 300MB/s hence 3s to 8s for this dataset):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18.8 ms, sys: 43.4 ms, total: 62.2 ms\n",
      "Wall time: 202 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:18]: \u001b[0mmemmap(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --targets=-1  \n",
    "# replace with one_engine_by_host.values()\n",
    "\n",
    "%time np.sum(big_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 1418.514432}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the first engine has now access to the data in memory and the free memory on the host has decreased by the same amount.\n",
    "\n",
    "We can now access this data from all the engines at once much faster as the disk will no longer be used: the shared memory buffer will instead accessed directly by all the engines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[stdout:0] \n",
      "CPU times: user 24.7 ms, sys: 21.1 ms, total: 45.8 ms\n",
      "Wall time: 61.5 ms\n",
      "[stdout:1] \n",
      "CPU times: user 25.2 ms, sys: 21.2 ms, total: 46.4 ms\n",
      "Wall time: 63.7 ms\n",
      "[stdout:2] \n",
      "CPU times: user 25 ms, sys: 21 ms, total: 46.1 ms\n",
      "Wall time: 70 ms\n",
      "[stdout:3] \n",
      "CPU times: user 15.8 ms, sys: 95 µs, total: 15.9 ms\n",
      "Wall time: 19.2 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[0:14]: \u001b[0mmemmap(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[1:15]: \u001b[0mmemmap(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[2:14]: \u001b[0mmemmap(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mOut[3:19]: \u001b[0mmemmap(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%px %time np.sum(big_mmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Alessandros-MacBook-Pro.local': 1426.706432}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_host_free_memory(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it seems that the engines have loaded a whole copy of the data but this actually not the case as the total amount of free memory was not impacted by the parallel access to the shared buffer. Furthermore, once the data has been preloaded from the hard drive using one process, all the of the other processes on the same host can access it almost instantly saving a lot of IO wait.\n",
    "\n",
    "This strategy makes it very interesting to load the readonly datasets of machine learning problems, especially when the same data is reused over and over by concurrent processes as can be the case when doing learning curves analysis or grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping Nested Numpy-based Data Structures with Joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "joblib is a utility library included in the sklearn package. Among other things it provides tools to serialize objects that comprise large numpy arrays and reload them as memmap backed datastructures.\n",
    "\n",
    "To demonstrate it, let's create an arbitrary python datastructure involving numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]], dtype=float32), array([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyDataStructure(object):\n",
    "    \n",
    "    def __init__(self, shape):\n",
    "        self.float_zeros = np.zeros(shape, dtype=np.float32)\n",
    "        self.integer_ones = np.ones(shape, dtype=np.int64)\n",
    "        \n",
    "data_structure = MyDataStructure((3, 4))\n",
    "data_structure.float_zeros, data_structure.integer_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now persist this datastructure to disk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_structure.pkl',\n",
       " 'data_structure.pkl_01.npy',\n",
       " 'data_structure.pkl_02.npy']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(data_structure, 'data_structure.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 alessandro  staff  273 Jun  6 09:50 data_structure.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff  128 Jun  6 09:50 data_structure.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff  176 Jun  6 09:50 data_structure.pkl_02.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls -l data_structure*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A memmapped copy of this datastructure can then be loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([[ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.]], dtype=float32), memmap([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memmaped_data_structure = joblib.load('data_structure.pkl', mmap_mode='r+')\n",
    "memmaped_data_structure.float_zeros, memmaped_data_structure.integer_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memmaping CV Splits for Multiprocess Dataset Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can leverage the previous tools to build a utility function that extracts Cross Validation splits ahead of time to persist them on the hard drive in a format suitable for memmaping by IPython engine processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data',\n",
    "    suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], n_iter=n_cv_iter,\n",
    "        test_size=test_size, random_state=random_state)\n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the digits dataset, we can run this from the :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/alessandro/zipfian/DSCI6007-instructor/week3/3_1_distrubted_systems/lecture/digits_cv_000.pkl',\n",
       " '/Users/alessandro/zipfian/DSCI6007-instructor/week3/3_1_distrubted_systems/lecture/digits_cv_001.pkl',\n",
       " '/Users/alessandro/zipfian/DSCI6007-instructor/week3/3_1_distrubted_systems/lecture/digits_cv_002.pkl',\n",
       " '/Users/alessandro/zipfian/DSCI6007-instructor/week3/3_1_distrubted_systems/lecture/digits_cv_003.pkl',\n",
       " '/Users/alessandro/zipfian/DSCI6007-instructor/week3/3_1_distrubted_systems/lecture/digits_cv_004.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "digits_split_filenames = persist_cv_splits(digits.data, digits.target,\n",
    "    name='digits', random_state=42)\n",
    "digits_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 alessandro  staff   304B Jun  6 09:50 digits_cv_000.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff   674K Jun  6 09:50 digits_cv_000.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff    11K Jun  6 09:50 digits_cv_000.pkl_02.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   225K Jun  6 09:50 digits_cv_000.pkl_03.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   3.6K Jun  6 09:50 digits_cv_000.pkl_04.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   304B Jun  6 09:50 digits_cv_001.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff   674K Jun  6 09:50 digits_cv_001.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff    11K Jun  6 09:50 digits_cv_001.pkl_02.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   225K Jun  6 09:50 digits_cv_001.pkl_03.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   3.6K Jun  6 09:50 digits_cv_001.pkl_04.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   304B Jun  6 09:50 digits_cv_002.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff   674K Jun  6 09:50 digits_cv_002.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff    11K Jun  6 09:50 digits_cv_002.pkl_02.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   225K Jun  6 09:50 digits_cv_002.pkl_03.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   3.6K Jun  6 09:50 digits_cv_002.pkl_04.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   304B Jun  6 09:50 digits_cv_003.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff   674K Jun  6 09:50 digits_cv_003.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff    11K Jun  6 09:50 digits_cv_003.pkl_02.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   225K Jun  6 09:50 digits_cv_003.pkl_03.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   3.6K Jun  6 09:50 digits_cv_003.pkl_04.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   304B Jun  6 09:50 digits_cv_004.pkl\r\n",
      "-rw-r--r--  1 alessandro  staff   674K Jun  6 09:50 digits_cv_004.pkl_01.npy\r\n",
      "-rw-r--r--  1 alessandro  staff    11K Jun  6 09:50 digits_cv_004.pkl_02.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   225K Jun  6 09:50 digits_cv_004.pkl_03.npy\r\n",
      "-rw-r--r--  1 alessandro  staff   3.6K Jun  6 09:50 digits_cv_004.pkl_04.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh digits*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the persisted CV splits can then be loaded back again using memmaping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = joblib.load(\n",
    "    'digits_cv_002.pkl',  mmap_mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([[  0.,   1.,  13., ...,   1.,   0.,   0.],\n",
       "       [  0.,   0.,   7., ...,   9.,   0.,   0.],\n",
       "       [  0.,   0.,   0., ...,  13.,   1.,   0.],\n",
       "       ..., \n",
       "       [  0.,   0.,   4., ...,  16.,   1.,   0.],\n",
       "       [  0.,   0.,   2., ...,  15.,   8.,   0.],\n",
       "       [  0.,   0.,   0., ...,   3.,   0.,   0.]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "memmap([5, 3, 1, ..., 8, 6, 4])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Parallel Model Selection and Grid Search\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leverage IPython.parallel and the Memory Mapping features of joblib to write a custom grid search utility that runs on cluster in a memory efficient manner.\n",
    "\n",
    "Assume that we want to reproduce the grid search from the previous session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([   0.1,    1. ,   10. ,  100. ]),\n",
      " 'gamma': array([  1.00000000e-04,   1.00000000e-03,   1.00000000e-02,\n",
      "         1.00000000e-01,   1.00000000e+00])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "pprint(svc_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GridSearchCV` internally uses the following `ParameterGrid` utility iterator class to build the possible combinations of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'C': 0.10000000000000001, 'gamma': 0.0001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.001},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.01},\n",
       " {'C': 0.10000000000000001, 'gamma': 0.10000000000000001},\n",
       " {'C': 0.10000000000000001, 'gamma': 1.0},\n",
       " {'C': 1.0, 'gamma': 0.0001},\n",
       " {'C': 1.0, 'gamma': 0.001},\n",
       " {'C': 1.0, 'gamma': 0.01},\n",
       " {'C': 1.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 1.0, 'gamma': 1.0},\n",
       " {'C': 10.0, 'gamma': 0.0001},\n",
       " {'C': 10.0, 'gamma': 0.001},\n",
       " {'C': 10.0, 'gamma': 0.01},\n",
       " {'C': 10.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 10.0, 'gamma': 1.0},\n",
       " {'C': 100.0, 'gamma': 0.0001},\n",
       " {'C': 100.0, 'gamma': 0.001},\n",
       " {'C': 100.0, 'gamma': 0.01},\n",
       " {'C': 100.0, 'gamma': 0.10000000000000001},\n",
       " {'C': 100.0, 'gamma': 1.0}]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.grid_search import ParameterGrid\n",
    "\n",
    "list(ParameterGrid(svc_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function to load the data from a CV split file and compute the validation score for a given parameter set and model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    validation_score = model.score(X_validation, y_validation)\n",
    "    return validation_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try on the digits dataset that we splitted previously as memmapable files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {\n",
    "    'C': np.logspace(-1, 2, 4),\n",
    "    'gamma': np.logspace(-4, 0, 5),\n",
    "}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, digits_split_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `grid_search` function is using the asynchronous API of the `LoadBalancedView`, we can hence monitor the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, we can introspect the completed task to find the best parameters set so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n",
      "[(0.99022222222222211, {'C': 1.0, 'gamma': 0.001}),\n",
      " (0.98888888888888893, {'C': 100.0, 'gamma': 0.001}),\n",
      " (0.98888888888888893, {'C': 10.0, 'gamma': 0.001}),\n",
      " (0.98755555555555552, {'C': 10.0, 'gamma': 0.0001}),\n",
      " (0.98711111111111127, {'C': 100.0, 'gamma': 0.0001})]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Trick: Truncated Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often wasteful to search all the possible combinations of parameters as done previously, especially if the number of parameters is large (e.g. more than 3).\n",
    "\n",
    "To speed up the discovery of good parameters combinations, it is often faster to randomized the search order and allocate a budget of evaluations, e.g. 10 or 100 combinations.\n",
    "\n",
    "See [this JMLR paper by James Bergstra](http://jmlr.csail.mit.edu/papers/v13/bergstra12a.html) for an empirical analysis of the problem. The interested reader should also have a look at [hyperopt](https://github.com/jaberg/hyperopt) that further refines this parameter search method using meta-optimizers.\n",
    "\n",
    "Randomized Parameter Search has just been implemented in the master branch of scikit-learn be part of the 0.14 release."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "StarCluster\n",
    "---\n",
    "\n",
    "![](https://camo.githubusercontent.com/a878477e72a1dc9fff694d9e5e2d6e743e934732/687474703a2f2f737461722e6d69742e6564752f636c75737465722f646f63732f6c61746573742f5f696d616765732f73636f766572766965772e706e67)\n",
    "\n",
    "StarCluster comes out of the STAR program at MIT. STAR stands for \"Software Tools for Academics and Researchers\". It is used to quickly provision a cluster of EC2 instances. Like EMR, it automatically configures them to be used as a cluster (rather than as independent machines) with one controller and many workers. Unlike EMR, it does not have a GUI. Also, Hadoop is just one of many plugins for StarCluster. The most important plugin, however, is IPython Cluster.\n",
    "\n",
    "<!--\n",
    "### Brian's Rant\n",
    "\n",
    "I 💀 the terms \"Master\" & \"Slave\" 😡 \n",
    "\n",
    "They are simply awful and have no place.\n",
    "\n",
    "Please use \"Leader\"/\"Coordinator\" and \"Follower\" / \"Worker\"\n",
    "-->\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "-----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
