{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Apache Spark\n",
    "============\n",
    "<img src=\"images/spark-logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ‚ÄúSpark is a Swiss Army knife of Big Data analytics tools‚Äù\n",
    "\n",
    "> ‚Äî Reynold Xin (@rxin), Berkeley AmpLab Development Lead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__When we learn a new framework__\n",
    "\n",
    "<img src=\"http://tclhost.com/vgTJ7Zi.gif\" style=\"width: 400px;\"/>\n",
    "\n",
    "[Source](http://thecodinglove.com/post/142839984399/when-we-learn-a-new-framework)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "What is Spark?\n",
    "------\n",
    "\n",
    "- Spark is a framework for distributed processing.\n",
    "\n",
    "- It is a streamlined alternative to MapReduce.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why learn Spark?\n",
    "---------\n",
    "\n",
    "- Spark enables you to analyze petabytes of data using a variety of methods (e.g., Streaming, SQL, and machine learning)\n",
    "- Spark is signficantly faster than MapReduce.\n",
    "- Paradoxically, Spark's API is simpler than the MapReduce API.\n",
    "- [Spark skills are in high demand (and pay well)](http://www.indeed.com/salary/q-Apache-Spark-Developer-l-San-Francisco,-CA.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this session, you will be able to:\n",
    "---\n",
    "\n",
    "- Understand the fundmental architecture and abstractions of Spark\n",
    "- Use the terminology: Resilient Distributed Dataset (RDD), transformation, and action\n",
    "- Create RDDs to distribute data across a cluster\n",
    "- Use Spark to analyze data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matei Zaharia\n",
    "-------------\n",
    "\n",
    "<img src=\"images/matei.jpg\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Essense of Spark (aka, the world's worst cologne)\n",
    "----------------\n",
    "\n",
    "What is the basic idea of Spark? __Leverage Memory__\n",
    "\n",
    "![](images/memory.png)\n",
    "\n",
    "- Spark takes the Map-Reduce paradigm and changes it in some critical\n",
    "  ways.\n",
    "\n",
    "- Instead of writing single Map-Reduce jobs a Spark job consists of a\n",
    "  series of map and reduce functions. \n",
    "  \n",
    "- However, the intermediate data is kept in memory instead of being\n",
    "  written to disk or written to HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/memory_detailed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "RTFM for Spark\n",
    "----\n",
    "\n",
    "- In-memory analytics, many times faster than Hadoop\n",
    "- A general-purpose computation framework that leverages distributed\n",
    "    - More flexible than MapReduce (it supports general execution graphs)\n",
    "    - Linear scalability and fault tolerance\n",
    "- Designed for running iterative algorithms \n",
    "- It supports a rich set of higher-level tools. Including:\n",
    "    - SparkSQL for \n",
    "    - MLlib for machine learning\n",
    "    - GraphX for graph processing\n",
    "    - Spark Streaming for realtime data processing\n",
    "- Highly compatible with Hadoop‚Äôs Storage APIs\n",
    "- Programming in Scala, Python, or Java       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Points to Ponder\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Since Spark keeps intermediate data in memory to get a speed, what does it make us give up? Where's the catch?\n",
    "</summary>\n",
    "1. Spark does a trade-off between memory and performance.\n",
    "<br>\n",
    "2. While Spark apps are faster, they also consume more memory.\n",
    "<br>\n",
    "3. Spark outshines Map-Reduce in iterative algorithms where the overhead of saving the results of each step to HDFS slows down Map-Reduce.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Architecture Simple\n",
    "---------------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Architecture More Complex\n",
    "---------------\n",
    "\n",
    "![](images/architecture2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Driver                 |Process that contains the Spark Context\n",
    "Executor               |Process that executes one or more Spark tasks\n",
    "Master                 |Process which manages applications across the cluster\n",
    "Worker                 |Process which manages executors on a particular worker node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Demo\n",
    "---------\n",
    "\n",
    "<img src=\"http://images.mentalfloss.com/sites/default/files/styles/article_640x430/public/flip-coin_5.jpg\" style=\"width: 400px;\"/>\n",
    "\n",
    "Flip a coin a \"big data\" number of times. What fraction of the time do you get heads?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark Install Sidebar\n",
    "----\n",
    "\n",
    "Installing and configing software suuuuuux!\n",
    "\n",
    "Try using Databrick's cloud. There is also local install..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: apache-spark-1.6.0 already installed\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "brew install apache-spark # Automtically builds the right version with pyspark!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ways to Launch\n",
    "----\n",
    "\n",
    "```shell\n",
    "# PySpark with IPython\n",
    "IPYTHON=1 pyspark\n",
    "\n",
    "# PySpark with IPython Notebook / Jupyter Notebook\n",
    "IPYTHON_OPTS=\"notebook\" pyspark \n",
    "\n",
    "# Spark with Scala REPL\n",
    "spark-shell --master local[*]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/brianspiering/anaconda/envs/py2_de/lib/python2.7/site-packages/IPython/utils/py3compat.py:288 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-518acaae9acb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/1.6.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/1.6.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway)\u001b[0m\n\u001b[1;32m    259\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 261\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/brianspiering/anaconda/envs/py2_de/lib/python2.7/site-packages/IPython/utils/py3compat.py:288 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`IPYTHON_OPTS=\"notebook\" pyspark` automagically make the Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x1117bb4d0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc #=> <pyspark.context.SparkContext at 0x10b477b10>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "509452\n",
      "51%\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "flips = 1000000\n",
    "heads = (sc.parallelize(xrange(flips))\n",
    "         .map(lambda i: random.random())\n",
    "         .filter(lambda r: r < 0.51)\n",
    "         .count())\n",
    "\n",
    "ratio = float(heads)/float(flips)\n",
    "\n",
    "print(heads)\n",
    "print(\"{:.0%}\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Highlights\n",
    "-----\n",
    "\n",
    "- `sc.parallelize` creates an RDD (more on that later)\n",
    "\n",
    "- `map` and `filter` are __transformations__ (functional fun!)\n",
    "\n",
    "- `count` is an __action__ and brings the data from the RDDs back to the\n",
    "  driver.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Context\n",
    "---\n",
    "\n",
    "There is only 1 context per cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop cluster\n",
    "sc.stop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo '1,Hillary,Clinton,hclinton@hotmail.com,Female,7.247.200.34\n",
    "2,Donald,Trump,the_boss_man@trump.com,Male,212.79.109.69\n",
    "3,Ted,Cruz,cruzing_with_ted@yahoo.com,Female,150.106.140.235\n",
    "4,Bernie,Sanders,sanders@freenet.org,Male,175.21.69.76' >logs.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new one with custom logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of 'Males': 2\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "logFile = \"logs.txt\"  # Should be some file on your system\n",
    "\n",
    "sc = SparkContext(\"local\", \"Simple App\")\n",
    "\n",
    "logData = sc.textFile(logFile).cache()\n",
    "male_count = logData.filter(lambda s: 'Male' in s).count()\n",
    "print(\"Number of 'Males': {}\").format(male_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Programming model \n",
    "-----\n",
    "\n",
    "Resilient Distributed Datasets (RDDs) are basic building blocks:\n",
    "\n",
    "- Distributed collection of objects, cached in-memory across cluster nodes\n",
    "- Automatically rebuilt on failure\n",
    "\n",
    "RDD operations:\n",
    "\n",
    "- Transformations: create new RDDs from existing ones\n",
    "- Actions: return a value to the master node after running a computation on the dataset\n",
    "\n",
    "Check out the [RDD docs](http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "RDD                    |*Resilient Distributed Dataset* or a distributed sequence of records\n",
    "Transformation         |Spark operation that produces an RDD\n",
    "Action                 |Spark operation that produces a local object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/api.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark Operations\n",
    "---\n",
    "\n",
    "Transformations:\n",
    "\n",
    "- map\n",
    "- filter\n",
    "- groupBy\n",
    "\n",
    "Actions:\n",
    "\n",
    "- count\n",
    "- collect\n",
    "- save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/transformations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/actions.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda (üòç)\n",
    "-------------------\n",
    "\n",
    "- Instead of `lambda` you can pass in fully defined functions into\n",
    "  `map`, `filter`, and other RDD transformations.\n",
    "\n",
    "- Use `lambda` for short functions. \n",
    "\n",
    "- Use `def` for more substantial functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common RDD Constructors\n",
    "-----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`sc.parallelize(list1)`                  |Create RDD of elements of list\n",
    "`sc.textFile(path)`                      |Create RDD of lines from file\n",
    "\n",
    "Common Transformations\n",
    "----------------------\n",
    "\n",
    "Expression                               |Meaning\n",
    "----------                               |-------\n",
    "`filter(lambda x: x % 2 == 0)`           |Discard non-even elements\n",
    "`map(lambda x: x * 2)`                   |Multiply each RDD element by `2`\n",
    "`map(lambda x: x.split())`               |Split each string into words\n",
    "`flatMap(lambda x: x.split())`           |Split each string into words and flatten sequence\n",
    "`sample(withReplacement=True,0.25)`      |Create sample of 25% of elements with replacement\n",
    "`union(rdd)`                             |Append `rdd` to existing RDD\n",
    "`distinct()`                             |Remove duplicates in RDD\n",
    "`sortBy(lambda x: x, ascending=False)`   |Sort elements in descending order\n",
    "\n",
    "\n",
    "Common Actions\n",
    "--------------\n",
    "\n",
    "Expression                             |Meaning\n",
    "----------                             |-------\n",
    "`collect()`                            |Convert RDD to in-memory list \n",
    "`take(3)`                              |First 3 elements of RDD \n",
    "`top(3)`                               |Top 3 elements of RDD\n",
    "`takeSample(withReplacement=True,3)`   |Create sample of 3 elements with replacement\n",
    "`sum()`                                |Find element sum (assumes numeric elements)\n",
    "`mean()`                               |Find element mean (assumes numeric elements)\n",
    "`stdev()`                              |Find element deviation (assumes numeric elements)\n",
    "\n",
    "Pop Quiz\n",
    "--------\n",
    "\n",
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,3,2,2,1]).distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: What will this output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 2, 2, 3]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,3,2,2,1]).sortBy(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create this input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "another line\n",
    "yet another line\n",
    "yet another another line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What do you get when you run this code?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "In this Spark job, what is the transformation and what is the action? \n",
    "`sc.parallelize(xrange(10)).filter(lambda x: x % 2 == 0).collect()`\n",
    "</summary>\n",
    "1. `filter` is the transformation.\n",
    "<br>\n",
    "2. `collect` is the action.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark Job\n",
    "---\n",
    "\n",
    "- A Spark job consists of a series of transformations followed by an\n",
    "  action.\n",
    "\n",
    "- It pushes the data to the cluster, all computation happens on the\n",
    "  *executors*, then the result is sent back to the driver.\n",
    "    \n",
    "Spark Terminology\n",
    "-----------------\n",
    "\n",
    "Term                   |Meaning\n",
    "----                   |-------\n",
    "Spark Job              |Sequence of transformations on data with a final action\n",
    "Spark Application      |Sequence of Spark jobs and other code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Finding Primes\n",
    "----\n",
    "\n",
    "Find all the primes less than 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_prime(number):\n",
    "    \"Determine if a number is prime\"\n",
    "    factor_min = 2\n",
    "    factor_max = int(number**0.5)+1\n",
    "    for factor in xrange(factor_min,factor_max):\n",
    "        if number % factor == 0:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to filter out non-primes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
     ]
    }
   ],
   "source": [
    "numbers = xrange(2,100)\n",
    "primes = sc.parallelize(numbers)\\\n",
    "            .filter(is_prime)\\\n",
    "            .collect()\n",
    "    \n",
    "print(primes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<img src=\"images/spark-cluster.png\">\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does `is_prime` execute?\n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Where does the RDD code execute?\n",
    "</summary>\n",
    "On the driver.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Map vs FlatMap\n",
    "----\n",
    "\n",
    "[Source](http://stackoverflow.com/questions/22350722/can-someone-explain-to-me-the-difference-between-map-and-flatmap-and-what-is-a-g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'hello', u'world'],\n",
       " [u'another', u'line'],\n",
       " [u'yet', u'another', u'line'],\n",
       " [u'yet', u'another', u'another', u'line']]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .map(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`map` transforms an RDD of length N into another RDD of length N."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FlatMap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'hello',\n",
       " u'world',\n",
       " u'another',\n",
       " u'line',\n",
       " u'yet',\n",
       " u'another',\n",
       " u'line',\n",
       " u'yet',\n",
       " u'another',\n",
       " u'another',\n",
       " u'line']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('input.txt') \\\n",
    "    .flatMap(lambda x: x.split()) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, `flatMap` (loosely speaking) transforms an RDD of length N into a collection of N collections, then flattens these into a single RDD of results.\n",
    "\n",
    "Data Scientists generally use `flatmap`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Word Count\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/word_count_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```java\n",
    "\n",
    "package org.apache.hadoop.examples;\n",
    " \n",
    "import java.io.IOException;\n",
    "import java.util.StringTokenizer;\n",
    " \n",
    "import org.apache.hadoop.conf.Configuration;\n",
    "import org.apache.hadoop.fs.Path;\n",
    "import org.apache.hadoop.io.IntWritable;\n",
    "import org.apache.hadoop.io.Text;\n",
    "import org.apache.hadoop.mapreduce.Job;\n",
    "import org.apache.hadoop.mapreduce.Mapper;\n",
    "import org.apache.hadoop.mapreduce.Reducer;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\n",
    "import org.apache.hadoop.mapreduce.lib.input.FileSplit;\n",
    "import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\n",
    "import org.apache.hadoop.util.GenericOptionsParser;\n",
    " \n",
    "public class WordCount {\n",
    " \n",
    "    public static class TokenizerMapper extends\n",
    "        Mapper<Object, Text, Text, IntWritable> {\n",
    " \n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    " \n",
    "        public void map(Object key, Text value, Context context)\n",
    "            throws IOException, InterruptedException {\n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            while (itr.hasMoreTokens()) {\n",
    "                word.set(itr.nextToken());\n",
    "                context.write(word, one);\n",
    "            }\n",
    "        }\n",
    "    }\n",
    " \n",
    "    public static class IntSumReducer extends\n",
    "        Reducer<Text, IntWritable, Text, IntWritable> {\n",
    " \n",
    "        private IntWritable result = new IntWritable();\n",
    " \n",
    "        public void reduce(Text key, Iterable<IntWritable> values,\n",
    "        Context context) throws IOException, InterruptedException {\n",
    "            int sum = 0;\n",
    "            for (IntWritable val : values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            result.set(sum);\n",
    "            context.write(key, result);\n",
    "        }\n",
    "    }\n",
    " \n",
    "    public static void main(String[] args) throws Exception {\n",
    "        Configuration conf = new Configuration();\n",
    "        String[] otherArgs = new GenericOptionsParser(conf, args)\n",
    "            .getRemainingArgs();\n",
    " \n",
    "        Job job = new Job(conf, \"word count\");\n",
    " \n",
    "        job.setJarByClass(WordCount.class);\n",
    " \n",
    "        job.setMapperClass(TokenizerMapper.class);\n",
    "        job.setCombinerClass(IntSumReducer.class);\n",
    "        job.setReducerClass(IntSumReducer.class);\n",
    " \n",
    "        job.setOutputKeyClass(Text.class);\n",
    "        job.setOutputValueClass(IntWritable.class);\n",
    " \n",
    "        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));\n",
    "        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));\n",
    " \n",
    "        System.exit(job.waitForCompletion(true) ? 0 : 1);\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from pyspark import SparkContext\n",
    " \n",
    "logFile = \"hdfs://localhost:9000/user/bigdatavm/input\"\n",
    " \n",
    "sc = SparkContext(\"spark://bigdata-vm:7077\", \"WordCount\")\n",
    " \n",
    "textFile = sc.textFile(logFile)\n",
    " \n",
    "wordCounts = textFile\\\n",
    "    .flatMap(lambda line: line.split())\\\n",
    "    .map(lambda word: (word, 1))\\\n",
    "    .reduceByKey(lambda a, b: a+b)\n",
    " \n",
    "wordCounts.saveAsTextFile(\"hdfs://localhost:9000/user/bigdatavm/output\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Architecture Revisited\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_detailed_overivew.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "- Spark is a general purpose, in-memory Big Data analytics framework\n",
    "- Beats down the elephant (Hadoop)\n",
    "- Simple API in Scala, Python, and Java\n",
    "- Functional, idiomatic programming style\n",
    "- Enables advanced programming models (e.g., SQL, machine learning, and graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "---\n",
    "Bonus Materials\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Value Pairs\n",
    "-----\n",
    "\n",
    "PairRDD\n",
    "-------\n",
    "\n",
    "At this point we know how to aggregate values across an RDD. If we\n",
    "have an RDD containing sales transactions we can find the total\n",
    "revenue across all transactions.\n",
    "\n",
    "Q: Using the following sales data find the total revenue across all\n",
    "transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing sales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'#ID    Date           Store   State  Product    Amount',\n",
       " u'101    11/13/2014     100     WA     331        300.00']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Split the lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#ID', u'Date', u'Store', u'State', u'Product', u'Amount'],\n",
       " [u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove `#`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'#ID', u'Date', u'Store', u'State', u'Product', u'Amount']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'101', u'11/13/2014', u'100', u'WA', u'331', u'300.00'],\n",
       " [u'104', u'11/18/2014', u'700', u'OR', u'329', u'450.00']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pick off last field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'300.00', u'450.00']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: x[-1])\\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert to float and then sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2230.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: float(x[-1]))\\\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReduceByKey\n",
    "-----------\n",
    "\n",
    "Q: Calculate revenue per state?\n",
    "\n",
    "- Instead of creating a sequence of revenue numbers we can create\n",
    "  tuples of states and revenue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 300.0),\n",
       " (u'OR', 450.0),\n",
       " (u'CA', 200.0),\n",
       " (u'CA', 330.0),\n",
       " (u'WA', 750.0),\n",
       " (u'CA', 200.0)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now use `reduceByKey` to add them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'CA', 730.0), (u'OR', 450.0), (u'WA', 1050.0)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Find the state with the highest total revenue.\n",
    "\n",
    "- You can either use the action `top` or the transformation `sortBy` then `take`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .sortBy(lambda state_amount:state_amount[1],ascending=False) \\\n",
    "    .take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'WA', 1050.0), (u'CA', 730.0), (u'OR', 450.0)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.textFile('sales.txt')\\\n",
    "    .map(lambda x: x.split())\\\n",
    "    .filter(lambda x: not x[0].startswith('#'))\\\n",
    "    .map(lambda x: (x[-3],float(x[-1])))\\\n",
    "    .reduceByKey(lambda amount1,amount2: amount1+amount2)\\\n",
    "    .top(3, key=lambda state_amount:state_amount[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What does `reduceByKey` do?\n",
    "</summary>\n",
    "1. It is like a reducer.\n",
    "<br>\n",
    "2. If the RDD is made up of key-value pairs, it combines the values\n",
    "   across all tuples with the same key by using the function we pass\n",
    "   to it.\n",
    "<br>\n",
    "3. It only works on RDDs made up of key-value pairs or 2-tuples.\n",
    "</details>\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- `reduceByKey` only works on RDDs made up of 2-tuples.\n",
    "\n",
    "- `reduceByKey` works as both a reducer and a combiner.\n",
    "\n",
    "- It requires that the operation is associative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Even more architecture\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_platform.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Cluster Manager\n",
    "---\n",
    "\n",
    "![](images/manager.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Logging\n",
    "-------------\n",
    "\n",
    "Q: How can I make Spark logging less verbose?\n",
    "\n",
    "- By default Spark logs messages at the `INFO` level.\n",
    "\n",
    "- Here are the steps to make it only print out warnings and errors.\n",
    "\n",
    "```sh\n",
    "cd $SPARK_HOME/conf\n",
    "cp log4j.properties.template log4j.properties\n",
    "```\n",
    "\n",
    "- Edit `log4j.properties` and replace `rootCategory=INFO` with `rootCategory=ERROR`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
