Spark Submit
-----

Being able to interact with data using the Spark REPL (or, in our case, Zeppelin notebook) is extremely valuable. But there are times when you want to just run a job as a batch process. In order to do this, we will use the [`spark-submit`](http://spark.apache.org/docs/latest/submitting-applications.html) utility. This should be run on the master node of your Spark cluster. (Bonus: install `spark-submit` locally and use it to submit your job to a remote cluster.)

Your deliverable for this lab is the python file you submit along with instructions on how to run it. Those instructions can either be in a comment block in your python file or in a separate README.
