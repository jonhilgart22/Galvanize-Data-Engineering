{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Serialization Frameworks\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Why do we need Serialization Frameworks?\n",
    "----\n",
    "\n",
    "I'm a _fancy_ Data Scientist and <3 my .csvs.\n",
    "\n",
    "However, distributed systems move __a lot__ data over the wire.\n",
    "\n",
    "We need to be efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "What are Serialization Frameworks?\n",
    "----\n",
    "\n",
    "![](http://www.codingeek.com/wp-content/uploads/2014/11/Serialization-deserialization-in-Java-Object-Streams.jpg)\n",
    "\n",
    "Serialization helps data in memory speak to the wire.\n",
    "\n",
    "Serialization is the process of translating data structures or object state into a format that can be stored and reconstructed later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this session you should know:\n",
    "----\n",
    "- The importance and important aspects of Serialization Frameworks\n",
    "- The common Big Data Serialization Frameworks:\n",
    "    - Protocol buffers\n",
    "    - Thrift\n",
    "    - Avro\n",
    "    - Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "You already knowSerialization Frameworks\n",
    "----\n",
    "\n",
    "__JSON__:\n",
    "\n",
    "````\n",
    "{\"menu\": {\n",
    "  \"id\": \"file\",\n",
    "  \"value\": \"File\",\n",
    "  \"popup\": {\n",
    "    \"menuitem\": [\n",
    "      {\"value\": \"New\", \"onclick\": \"CreateNewDoc()\"},\n",
    "      {\"value\": \"Open\", \"onclick\": \"OpenDoc()\"},\n",
    "      {\"value\": \"Close\", \"onclick\": \"CloseDoc()\"}\n",
    "    ]\n",
    "  }\n",
    "}}\n",
    "````\n",
    "\n",
    "__Pickle__:\n",
    "\n",
    "![](https://www.safaribooksonline.com/library/view/head-first-python/9781449397524/httpatomoreillycomsourceoreillyimages1368712.png.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "What are the limitations of JSON and pickles?\n",
    "</summary>\n",
    "\n",
    "JSON is \"heavy\". For example, precision of numbers can not be specified.\n",
    "\n",
    "<br>\n",
    "Pickle is Python specific (even Python version specific)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Graph schemas sidebar\n",
    "----\n",
    "\n",
    "Graphs are nice CS ways of modeling the world\n",
    "\n",
    "### Elements of a graph schema\n",
    "![Visualizing the relationship between FaceSpace facts](https://s3-us-west-2.amazonaws.com/dsci6007/assets/fig2-16.png)\n",
    "* Nodes are the entities in the system.\n",
    "* Edges are relationships between nodes.\n",
    "* Properties are information about entities.\n",
    "\n",
    "### The need for an enforceable schema\n",
    "Suppose you chose to represent Tom’s age using JSON:\n",
    "```json\n",
    "{\"id\": 3, \"field\":\"age\", \"value\":28, \"timestamp\":1333589484}\n",
    "```\n",
    "There’s no way to ensure that all subsequent facts will follow the same format.\n",
    "```json\n",
    "{\"name\": \"Alice\", \"field\":\"age\", \"value\":25, \"timestamp\":\"2012/03/29 08:12:24\"}\n",
    "{\"id\":2, \"field\":\"age\", \"value\":36}\n",
    "```\n",
    "Both of these examples are valid JSON, but they have inconsistent formats or missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "\"Advanced\"  Serialization Frameworks\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Protocol buffers\n",
    "---\n",
    "\n",
    "![](http://i62.tinypic.com/otfel3.jpg)\n",
    "\n",
    "Protocol buffers are Google's \n",
    "- language-neutral\n",
    "- platform-neutral\n",
    "- extensible mechanism for serializing structured data\n",
    "\n",
    "Think XML, but smaller, faster, and simpler. \n",
    "\n",
    "You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.\n",
    "\n",
    "\n",
    "```\n",
    "message Person {\n",
    "  required string name = 1;\n",
    "  required int32 id = 2;\n",
    "  optional string email = 3;\n",
    "}\n",
    "```\n",
    "\n",
    "```\n",
    "Person brian = Person.newBuilder()\n",
    "    .setId(42)\n",
    "    .setName(\"Brian Spiering\")\n",
    "    .setEmail(\"brian.spiering@galvanize.com\")\n",
    "    .build();\n",
    "output = new FileOutputStream(args[0]);\n",
    "john.writeTo(output);\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "----\n",
    "\n",
    "<details><summary>\n",
    "When don't you want to use a protocol buffer?'\n",
    "</summary>\n",
    "\n",
    "- You aren’t prepared to tie the data model to a schema\n",
    "- You don’t have the bandwidth to add another tool to your arsenal\n",
    "- You need or want data to be human readable\n",
    "- Data from the service is directly consumed by a web browser\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Apache Thrift \n",
    "---\n",
    "\n",
    "![](http://www.symfony-project.org/uploads/plugins/f4618597b8cf4bdd736298718bf66af5.png)\n",
    "\n",
    "Allows different components talk to each other\n",
    "\n",
    "Google Translate for machines and services\n",
    "\n",
    "---\n",
    "What does Thrift do?\n",
    "----\n",
    "\n",
    "- Quickly define your service\n",
    "- Compiles client and server wrappers for API calls\n",
    "- Makes all networking, serialization transparent\n",
    "- Takes care of low-level, repetitive details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Apache Avro\n",
    "----\n",
    "\n",
    "![](https://avro.apache.org/images/avro-logo.png)\n",
    "\n",
    "Avro stores both the data definition and the data together in one message or file making it easy for programs to dynamically understand the information. \n",
    "\n",
    "1. Store data defintion (data types and protocols) in JSON \n",
    "2. Store the data itself compact and efficient binary format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/avro.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primitive Types\n",
    "The set of primitive type names is:  \n",
    "* `null`: no value\n",
    "* `boolean`: a binary value\n",
    "* `int`: 32-bit signed integer\n",
    "* `long`: 64-bit signed integer\n",
    "* `float`: single precision (32-bit) IEEE 754 floating-point number\n",
    "* `double`: double precision (64-bit) IEEE 754 floating-point number\n",
    "* `bytes`: sequence of 8-bit unsigned bytes\n",
    "* `string`: unicode character sequence  \n",
    "\n",
    "Primitive types have no specified attributes.\n",
    "\n",
    "Primitive type names are also defined type names. Thus, for example, the schema \"string\" is equivalent to:\n",
    "\n",
    "    {\"type\": \"string\"}\n",
    "\n",
    "#### Complex Types\n",
    "Avro supports six kinds of complex types: `records`, `enums`, `arrays`, `maps`, `unions` and `fixed`.\n",
    "\n",
    "See http://avro.apache.org/docs/current/spec.html for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fun Fact\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "Why does Avro work better for Hadoop?\n",
    "</summary>\n",
    " Avro files include markers that can be used to splitting large data sets into subsets suitable for MapReduce processing\n",
    "<br>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import avro.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PersonID = [{\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PersonID1\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"cookie\",\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PersonID2\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"user_id\",\n",
    "                \"type\": \"long\"\n",
    "            }\n",
    "        ]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PageID = [{\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PageID\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"url\",\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        ]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Nodes = PersonID + PageID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EquivEdge = {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"EquivEdge\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id1\",\n",
    "                \"type\": [\n",
    "                    \"PersonID1\",\n",
    "                    \"PersonID2\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"id2\",\n",
    "                \"type\": [\n",
    "                    \"PersonID1\",\n",
    "                    \"PersonID2\"\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PageViewEdge = {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PageViewEdge\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"person\",\n",
    "                \"type\": [\n",
    "                    \"PersonID1\",\n",
    "                    \"PersonID2\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"page\",\n",
    "                \"type\": \"PageID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"nonce\",\n",
    "                \"type\": \"long\"\n",
    "            }\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Edges = [EquivEdge, PageViewEdge]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Page Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PageProperties = [{\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PagePropertyValue\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"page_views\",\n",
    "                \"type\": \"int\"\n",
    "            }\n",
    "        ]\n",
    "    }, \n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PageProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": \"PageID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": \"PagePropertyValue\"\n",
    "            }\n",
    "        ]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PageProperties = [{\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PageProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": \"PageID\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": {\n",
    "                    \"type\": \"record\",\n",
    "                    \"name\": \"PagePropertyValue\",\n",
    "                    \"fields\": [\n",
    "                        {\n",
    "                            \"name\": \"page_views\",\n",
    "                            \"type\": \"int\"\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Person Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PersonProperties = [\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Location\",\n",
    "        \"fields\": [\n",
    "            {\"name\": \"city\", \"type\": [\"string\", \"null\"]},\n",
    "            {\"name\": \"state\", \"type\": [\"string\", \"null\"]},\n",
    "            {\"name\": \"country\", \"type\": [ \"string\",\"null\"]}\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"enum\",\n",
    "        \"name\": \"GenderType\",\n",
    "        \"symbols\": [\"MALE\", \"FEMALE\"]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"PersonProperty\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"id\",\n",
    "                \"type\": [\n",
    "                    \"PersonID1\",\n",
    "                    \"PersonID2\"\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"property\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"PersonPropertyValue1\",\n",
    "                        \"fields\": [{\"name\": \"full_name\", \"type\": \"string\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"PersonPropertyValue2\",\n",
    "                        \"fields\": [{\"name\": \"gender\", \"type\": \"GenderType\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"PersonPropertyValue3\",\n",
    "                        \"fields\": [{\"name\": \"location\", \"type\": \"Location\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tying everything together into data objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Data = [\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Pedigree\",\n",
    "        \"fields\": [{\"name\": \"true_as_of_secs\", \"type\": \"int\"}]\n",
    "    },\n",
    "    {\n",
    "        \"namespace\": \"analytics.avro\",\n",
    "        \"type\": \"record\",\n",
    "        \"name\": \"Data\",\n",
    "        \"fields\": [\n",
    "            {\n",
    "                \"name\": \"pedigree\",\n",
    "                \"type\": \"Pedigree\"\n",
    "            },\n",
    "            {\n",
    "                \"name\": \"dataunit\",\n",
    "                \"type\": [\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit1\",\n",
    "                        \"fields\": [{\"name\": \"person_property\", \"type\": \"PersonProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit2\",\n",
    "                        \"fields\": [{\"name\": \"page_property\", \"type\": \"PageProperty\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit3\",\n",
    "                        \"fields\": [{\"name\": \"equiv\", \"type\": \"EquivEdge\"}]\n",
    "                    },\n",
    "                    {\n",
    "                        \"type\": \"record\",\n",
    "                        \"name\": \"DataUnit4\",\n",
    "                        \"fields\": [{\"name\": \"page_view\", \"type\": \"PageViewEdge\"}]\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "schema = avro.schema.parse(json.dumps(Nodes + Edges + PageProperties + PersonProperties + Data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Parquet\n",
    "---\n",
    "\n",
    "![](http://www.bauwerk-parkett.com/parquet-images/floor/1200x800/2444/parquet-acacia-monopark-steamed-strip-470x70x96mm.jpg)\n",
    "\n",
    "![](https://pbs.twimg.com/profile_images/474255479032385537/OGYr_m6J.jpeg)\n",
    "\n",
    "Parquet ia an efficient data store for analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/parquet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Graph Schema Flashback\n",
    "---\n",
    "\n",
    "![](images/schema_columns.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Parquet Features\n",
    "---\n",
    "\n",
    "- __Columnar__ File Format\n",
    "- Not tied to any commerical framework\n",
    "- Supports Nested Data Structures\n",
    "- Accessible by HIVE, Spark, Pig, Drill, MapReduce (MR)\n",
    "- R/W in HDFS or local file system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Columnar FTW!\n",
    "---\n",
    "\n",
    "Features\n",
    "\n",
    "- Limits IO to data needed\n",
    "- Compresses better\n",
    "- Type specific encodings available\n",
    "- Enables vectorized execution engines\n",
    "\n",
    "![](images/layout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/read.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/encoding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Delta Encoding Deep Dive\n",
    "---\n",
    "\n",
    "![](images/delta.png)\n",
    "\n",
    "[Go way do the rabbt hole](http://www.slideshare.net/julienledem/parquet-hadoop-summit-2013)\n",
    "\n",
    "WARNING: bit packing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "\n",
    "<details><summary>\n",
    "Where else have we seen delta encoding?\n",
    "</summary>\n",
    "git/GitHub\n",
    "<br>\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/file.png)\n",
    "\n",
    "![](images/file_format.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended Reading: \n",
    "- [Dremel made simple with parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)\n",
    "- [Understanding parquet](https://dzone.com/articles/understanding-how-parquet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Common Workflows\n",
    "----\n",
    "\n",
    "![](images/workflow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Out of Scope\n",
    "---\n",
    "\n",
    "### Actor Model\n",
    "\n",
    "The actor model is a programming model for concurrency in a single process. \n",
    "\n",
    "This hugely important. The most interesting, cutting-edge systems are using this. \n",
    "\n",
    "Check out the [Akkka Actor Model](http://doc.akka.io/docs/akka/current/general/actors.html)\n",
    "\n",
    "---\n",
    "\n",
    "### Schema Evolution\n",
    "\n",
    "![](images/schema_break.png)\n",
    "\n",
    "Things change. You will break stuff. Have a plan. Prepare to suffer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "-------------------------------------------------\n",
    "\n",
    "- Serialization Frameworks are efficent and common ways of transfering and storing data.\n",
    "- It is important to have schema and have it be self-describing and flexible.\n",
    "- Protocol buffers and Thrift are general, language-agnostic ways of encoding data with schemas.\n",
    "- Avro also specifies the structure of the data being encoded and isbetter suited to Hadoop.\n",
    "- Parquet is the best for Data Science workloads. Compact way to store data for efficent analytic querying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
