{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hadoop\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/hadoop_thing.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Why Hadoop?\n",
    "----\n",
    "\n",
    "Big Data Problem\n",
    "----------------\n",
    "\n",
    "We have a 100 TB of sales data that looks like this:\n",
    "\n",
    "ID    |Date          |Store  |State |Product   |Amount\n",
    "--    |----          |-----  |----- |-------   |------\n",
    "101   |11/13/2014    |100    |WA    |331       |300.00\n",
    "104   |11/18/2014    |700    |OR    |329       |450.00\n",
    "\n",
    "What If\n",
    "-------\n",
    "\n",
    "What are some of the questions we could answer if we could process this huge data set?\n",
    "\n",
    "- How much revenue did we make by store, state?\n",
    "\n",
    "- How much revenue did we make by product?\n",
    "\n",
    "- How much revenue did we make by week, month, year?\n",
    "\n",
    "Statistical Uses\n",
    "----------------\n",
    "\n",
    "Why are these interesting?\n",
    "\n",
    "- These questions can help us figure out which products are selling\n",
    "  in which markets, at what time of the year.\n",
    "\n",
    "Engineering Problem\n",
    "-------------------\n",
    "\n",
    "To answer these questions we have to solve two problems:\n",
    "\n",
    "- Store 100 TB of data\n",
    "\n",
    "- Process 100 TB of data\n",
    "\n",
    "Here is our starting point:\n",
    "\n",
    "- To solve this problem we have been provided with 1000 commodity Linux servers.\n",
    "\n",
    "- How can we organize these machines to store and process this data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this class, we will be able to explain:\n",
    "----------\n",
    "\n",
    "- What problem Hadoop solves\n",
    "- The basic abstraction of Hadoop\n",
    "- What is HDFS and how it works\n",
    "\n",
    "Explain how HDFS splits up large files into blocks and stores them\n",
    "  on a cluster.\n",
    "  \n",
    "- Explain how HDFS uses replication to ensure fault tolerance of the\n",
    "  data.\n",
    "\n",
    "- Explain how HDFS uses `fsimage` and `edits` files to ensure fault\n",
    "  tolerance of the metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tell the story about the distrubted processing in lab__:\n",
    "- Not having a server\n",
    "- Using PC, MATLAB, & dropbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Hadoop Intro\n",
    "---\n",
    "\n",
    "![](http://e-nouri.com/wp-content/uploads/2014/01/hadoop-pic11.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop is a cluster operating system. It is made up of:\n",
    "\n",
    "- HDFS, which coordinates storing large amounts of data on a\n",
    "  cluster.\n",
    "\n",
    "- MapReduce which coordinates processing data across a cluster of\n",
    "  machines.\n",
    "\n",
    "Google Papers\n",
    "-------------\n",
    "\n",
    "Hadoop, HDFS, and MapReduce are open source implementations of the\n",
    "ideas in these papers from Google and Stanford.\n",
    "\n",
    "- Paper #1: [2003] The Google File System     \n",
    "    <http://research.google.com/archive/gfs-sosp2003.pdf>\n",
    "\n",
    "- Paper #2: [2004] MapReduce: Simplified Data Processing on Large Clusters    \n",
    "    <http://research.google.com/archive/mapreduce-osdi04.pdf>\n",
    "\n",
    "- Paper #3: [2006] Bigtable: A Distributed Storage System for Structured Data\n",
    "    <http://static.googleusercontent.com/media/research.google.com/en/us/archive/bigtable-osdi06.pdf>\n",
    "\n",
    "\n",
    "Doug Cutting\n",
    "------------\n",
    "\n",
    "<img src=\"images/doug-cutting.png\">\n",
    "\n",
    "Hadoop\n",
    "------\n",
    "\n",
    "<img style=\"width:50%\" src=\"images/yellow-elephant-hadoop.jpg\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "<img src=\"images/devastator-transformer.jpg\">\n",
    "\n",
    "Hadoop Analogy\n",
    "--------------\n",
    "\n",
    "System     |Analogy\n",
    "------     |-------\n",
    "Hadoop     |Cluster Operating System\n",
    "HDFS       |Cluster Disk Drive\n",
    "MapReduce  |Cluster CPU\n",
    "\n",
    "- Hadoop clusters are made up of commodity Linux machines.\n",
    "\n",
    "- Each machine is weak and limited.\n",
    "\n",
    "- Hadoop combines these machines.\n",
    "\n",
    "- The Hadoop cluster is bigger and more powerful than the individual\n",
    "  machines.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hadoop / HDFS design\n",
    "---\n",
    "\n",
    "![](images/hdfs_architecture.png)\n",
    "\n",
    "Single NameNode for managing FS metadata\n",
    "\n",
    "Multiple DataNode for storing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "HDFS Daemons\n",
    "------------\n",
    "\n",
    "Daemon Name          |Role                              |Number Deployed\n",
    "-----------          |-----------                       |---------------\n",
    "NameNode             |Manages DataNodes and metadata    |1 per cluster\n",
    "Secondary NameNode   |Compacts recovery metadata        |1 per cluster\n",
    "Standby NameNode     |NameNode backup                   |1 per cluster\n",
    "DataNode             |Stores/processes file parts       |1 per worker machine\n",
    "\n",
    "HDFS Cluster\n",
    "------------\n",
    "\n",
    "![](images/hadoop_arch.png)\n",
    "\n",
    "The __Secondary NameNode__ is confusing. It is __NOT__ the backup to the name node but instead provides additional functionality for the HDFS state logs managed primarily by the primary name node.  There are \"checkpoint\" and \"backup\" nodes which perform what one might think of as failover for the primary name node\n",
    "\n",
    "[Source](http://hadoop.apache.org/docs/r2.7.2/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html#Secondary_NameNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/storage.png)\n",
    "\n",
    "http://www.coderanch.com/t/653229/hadoop/databases/Default-block-size-HDFS\n",
    "\n",
    "![](images/breaking_it_up.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Hadoop version sidebar\n",
    "----\n",
    "There are 2 major version of Hadoop\n",
    "1.* The orginal version with just HSDF & MapReduce\n",
    "2.* The version allow for more extensibility (more in upcoming lecture)\n",
    "\n",
    "Block size was 64 MB  in 1.* \n",
    "Block size is now __128 MB__ in 2.* \n",
    "\n",
    "Student Question: Why would they double block size?\n",
    "\n",
    "[source](http://www.coderanch.com/t/653229/hadoop/databases/Default-block-size-HDFS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/Writing-Files-to-HDFS.png)\n",
    "\n",
    "![](images/Hadoop-Rack-Awareness.png)\n",
    "\n",
    "[Source](http://bradhedlund.com/2011/09/10/understanding-hadoop-clusters-and-the-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "HDFS Command Line Access\n",
    "-----\n",
    "\n",
    "Command                                  |Meaning\n",
    "-------                                  |-------\n",
    "`hadoop fs -ls hdfs://nn:8020/user/jim`  |List home directory of user `jim`\n",
    "`hadoop fs -ls /user/jim`                |List home directory of user `jim`\n",
    "`hadoop fs -ls data`                     |List home directory of user `jim`\n",
    "`hadoop fs -ls `                         |List home directory of user `jim`\n",
    "`hadoop fs -mkdir dir`                   |Make new directory `/user/jim/dir`\n",
    "`hadoop fs -mkdir -p a/b/c/dir`          |Make new directory and all missing parents\n",
    "`hadoop fs -rm file`                     |Remove `/user/jim/file`\n",
    "`hadoop fs -rm -r dir`                   |Remove `/user/jim/dir` and all its contents\n",
    "`hadoop fs -rm dir`                      |Remove `/user/jim/dir` if it is empty\n",
    "`hadoop fs -put file1 file2`             |Copy local `file1` to `/user/jim/file2`\n",
    "`hadoop fs -put file1 dir/`              |Copy local `file1` to `/user/jim/dir/file2`\n",
    "`echo 'hi' `&#124;` hadoop fs -put - file.txt`|Put string `hi` into `/user/jim/file.txt`\n",
    "`hadoop fs -get /user/jim/file.txt`      |Copy `/user/jim/file.txt` to local `file.txt`\n",
    "`hadoop fs -get /user/jim/file1 file2`   |Copy `/user/jim/file1` to local `file2`\n",
    "`hadoop fs -cat /user/jim/file.txt`      |Cat `/user/jim/file.txt` to stdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of the streaming put?\n",
    "</summary>\n",
    "1. The data does not have to be staged anywhere.<br>\n",
    "2. You can put data into HDFS from a running program.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Can you access HDFS files on a remote cluster?\n",
    "</summary>\n",
    "1. Use `hadoop fs -ls hdfs://remote-nn:8020/user/jim/path`<br>\n",
    "2. Use `hadoop fs -Dfs.defaultFS=hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "3. Use `hadoop fs -fs hdfs://remote-nn:8020 /user/jim/path`<br>\n",
    "</details>\n",
    "\n",
    "Files Blocks Replicas\n",
    "---------------------\n",
    "\n",
    "<img src=\"images/hdfs-files-blocks-replicas.png\">\n",
    "\n",
    "- Each file is made up of blocks.\n",
    "\n",
    "- Each block has 3 copies or *replicas*.\n",
    "\n",
    "- The replicas are equivalent---there is no primary replica.\n",
    "\n",
    "- The block size for a file cannot be changed once a file is created.\n",
    "\n",
    "- The replication for a file *can* be changed dynamically.\n",
    "\n",
    "\n",
    "Custom Block Size and Replication\n",
    "---------------------------------\n",
    "\n",
    "Command                                          |Meaning\n",
    "-------                                          |-------\n",
    "`hadoop fs -D dfs.blocksize=67108864 -put file`  |Put `file` with block size 64MB \n",
    "`hadoop fs -D dfs.replication=1 -put file`       |Put `file` with replication 1\n",
    "`hadoop fs -setrep 1 file`                       |Change replication of `file` to 1\n",
    "`hadoop fs -setrep -R 1 dir`                     |Change replication of `dir` and contents to 1\n",
    "`hadoop fs -setrep -w 1 file`                    |Change replication and block till done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "File Security\n",
    "-------------\n",
    "\n",
    "Command                               |Meaning\n",
    "-------                               |-------\n",
    "`hadoop fs -chown jim file1`          |Change owner of `file1` to `jim`\n",
    "`hadoop fs -chgrp staff file1`        |Change group of `file1` to `staff`\n",
    "`hadoop fs -chown jim:staff file1`    |Change both owner and group\n",
    "`hadoop fs -chown -R jim:staff dir1`  |Change both for `dir1` and its contents\n",
    "`hadoop fs -chgrp -R staff dir1`      |Change group of `dir1` and its contents\n",
    "`hadoop fs -chmod 755 file1`          |Set `file1` permissions to `rwxr-xr-x`\n",
    "`hadoop fs -chmod 755 -R dir1`        |Set permissions for `dir1` and its contents\n",
    "\n",
    "Hadoop Security\n",
    "---------------\n",
    "\n",
    "Q: What is the primary HDFS security model?\n",
    "\n",
    "- HDFS uses the Unix file system security model.\n",
    "\n",
    "- Unix secures access through authentication and authorization.\n",
    "\n",
    "- Authentication: Who are you.\n",
    "\n",
    "- Authorization: Who has access to a file.\n",
    "\n",
    "- By default HDFS enforces authorization but not authentication.\n",
    "\n",
    "Q: How can I impersonate someone to hack their files in HDFS? \n",
    "\n",
    "- `sudo -u jim hadoop fs -cat /user/jim/deep-dark-secrets.txt`\n",
    "\n",
    "Q: How can I secure the system against impersonation?\n",
    "\n",
    "- To fix this you have to enable Kerberos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "What is HDFS good for?\n",
    "---\n",
    "\n",
    "- Large files\n",
    "- Streaming data access\n",
    "- Read-centric loads\n",
    "\n",
    "### NOT great for:\n",
    "\n",
    "- Lots of small files\n",
    "- Random access\n",
    "- Low-latency access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "----\n",
    "MapReduce\n",
    "-----\n",
    "\n",
    "Q: What is MapReduce?\n",
    "\n",
    "- MapReduce is a system for processing data on HDFS.\n",
    "\n",
    "- In the *map* phase data is processed locally, with one *mapper* per\n",
    "  block.\n",
    "\n",
    "- In the *reduce* phase the results of the map phase are consolidated.\n",
    "\n",
    "\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What is the advantage of running `grep` as a MapReduce program?\n",
    "</summary>\n",
    "`grep` on MapReduce will scan blocks in parallel and complete\n",
    "faster.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "---\n",
    "\n",
    "- Understand the power of having standardized framework for distrbuted processing\n",
    "- What Hadoop was orginally: HDFS & MapReduce\n",
    "- The fundementals of HDFS\n",
    "- MapReduce teaser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
