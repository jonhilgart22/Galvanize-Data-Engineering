{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://kodcu.com/wp/wp-content/uploads/2014/06/mllib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Build ML Pipelines\n",
    "- Recongize when to use Spark Packages\n",
    "- Perform fundamental text processing:\n",
    "    + tf-idf\n",
    "    + word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ML Pipelines\n",
    "---\n",
    "\n",
    "![](images/pipelines.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML pipelines combine multiple algorithms into a single pipeline, or workflow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Review\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is a DataFrame?\n",
    "</summary>\n",
    "Spark ML uses DataFrame from Spark SQL as an ML dataset, which can hold a variety of data types.\n",
    "<br>\n",
    "E.g., a DataFrame could have different columns storing text, feature vectors, true labels, and predictions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "---\n",
    "Pipeline components\n",
    "----\n",
    "\n",
    "__Transformer__: A Transformer is an algorithm which can transform one DataFrame into another DataFrame. E.g., an ML model is a Transformer which transforms a DataFrame with features into a DataFrame with predictions.\n",
    "\n",
    "__Estimator__: An Estimator is an algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model.\n",
    "\n",
    "__Pipeline__: A Pipeline chains multiple Transformers and Estimators together to specify an ML workflow.\n",
    "\n",
    "__Parameter__: All Transformers and Estimators now share a common API for specifying parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/images/spark-mllib-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![](https://databricks.com/wp-content/uploads/2015/01/pipeline-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/ml-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "[Pipelines Demo](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6058142077065523/505261678981876/4338926410488997/latest.html)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Hyper-parameter Tuning\n",
    "---\n",
    "\n",
    "```python\n",
    "# Build a parameter grid.\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n",
    "                .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "                .build()\n",
    "\n",
    "# Set up cross-validation.\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3) \n",
    "                          \n",
    "# Fit a model with cross-validation.\n",
    "cvModel = crossval.fit(training)\n",
    "```\n",
    "see http://spark.apache.org/docs/latest/ml-guide.html  \n",
    "and https://github.com/apache/spark/blob/master/examples/src/main/python/ml/cross_validator.py#L69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark 2.0 has model persistence, aka saving and loading\n",
    "----\n",
    "\n",
    "```python\n",
    "# Define the workflow\n",
    "rf = RandomForestClassifier()\n",
    "cv = CrossValidator(estimator=rf, ...)\n",
    "\n",
    "# Fit the model, running Cross-Validation\n",
    "cvModel = cv.fit(trainingData)\n",
    "\n",
    "# Extract the results, i.e., the best Random Forest model\n",
    "bestModel = cvModel.bestModel\n",
    "\n",
    "# Save the RandomForest model\n",
    "bestModel.save(\"rfModelPath\")\n",
    "```\n",
    "\n",
    "[Source](https://databricks.com/blog/2016/05/31/apache-spark-2-0-preview-machine-learning-model-persistence.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "But what about \"Streaming foo bar on baz\" algorithm?\n",
    "---\n",
    "\n",
    "It was invented yesterday by Google. They haven't release any code. There is just a fuzzy screenshot of Jeff Dean's desktop, but I __can't__ do my capstone without it!\n",
    "\n",
    "![](http://i.imgur.com/SK9VCDJ.jpg)\n",
    "\n",
    "Have you heard of [Spark Packages](http://spark-packages.org/)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://static1.fjcdn.com/thumbnails/comments/Thank+you+kind+sir+_621c0f81885b90188ac2580876cb77f2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "Text Processing in MLlib\n",
    "----\n",
    "\n",
    "![](http://image.slidesharecdn.com/introtobigdataandhadoop01-141116061412-conversion-gate02/95/an-introduction-to-bigdata-processing-applying-hadoop-4-638.jpg?cb=1417344122)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Review\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is the goal of Natural Language Processing (NLP)?\n",
    "</summary>\n",
    "Try strings into numbers, than apply standard machine learning algorithms\n",
    "</details>\n",
    "<br>\n",
    "<br>\n",
    "<details><summary>\n",
    "What are the most common text algorithms?\n",
    "</summary>\n",
    "1. Word Count (including ngrams)  \n",
    "2. tf-idf  \n",
    "3. word2vec\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "tf-idf demo\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import HashingTF, IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [[\"a\", \"a\", \"b\"], \n",
    "        [\"a\", \"b\", \"c\"], \n",
    "        [\"a\", \"a\", \"d\"]]\n",
    "rdd = sc.parallelize(data, numSlices=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {31: 1.0, 44: 2.0}),\n",
       " SparseVector(100, {14: 1.0, 31: 1.0, 44: 1.0}),\n",
       " SparseVector(100, {1: 1.0, 44: 2.0})]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = HashingTF(numFeatures=100)\n",
    "doc = \"a a b\".split(\" \")\n",
    "tfs = tf.transform(rdd) #=> (numFeatures, {term_index, term_frequency})\n",
    "tfs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SparseVector(100, {31: 0.2877, 44: 0.0}),\n",
       " SparseVector(100, {14: 0.6931, 31: 0.2877, 44: 0.0}),\n",
       " SparseVector(100, {1: 0.6931, 44: 0.0})]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf = IDF().fit(tfs)\n",
    "tfidf = idf.transform(tfs)\n",
    "tfidf.collect() #=> (numFeatures, {term_index, tf-idf})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What is tf-idf useful for?\n",
    "</summary>\n",
    "The \"real\" importance of a term for a give corpus\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "word2vec\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.feature import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inp = (sc\n",
    "       .textFile(\"grimms_fairy_tales.txt\")\n",
    "       .map(lambda row: row.split(\" \"))\n",
    "        ) # Have to keep context for word2vec\n",
    "\n",
    "word2vec = Word2Vec()\n",
    "model = word2vec.fit(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dwarf: 0.567\n",
      "morning: 0.567\n",
      "wind: 0.558\n",
      "princess: 0.556\n",
      "table: 0.556\n"
     ]
    }
   ],
   "source": [
    "n = 5\n",
    "synonyms = model.findSynonyms('king', n)\n",
    "\n",
    "for word, cosine_distance in synonyms:\n",
    "    print(\"{}: {:.3}\".format(word, cosine_distance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[RTFM for PySpark NLP](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.feature.HashingTF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What do the rankings change? What is the best way to prevent that?\n",
    "</summary>\n",
    "The weights of the network are randomized differently each time. <br>\n",
    "Train the model on more data to stablize the learning.\n",
    "</details>\n",
    "<br>\n",
    "<br>\n",
    "<details><summary>\n",
    "What other word2vec functions are missing?\n",
    "</summary>\n",
    "Almost all of them! <br>\n",
    "For example, \"doesn't match\" for a group <br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Summary\n",
    "----\n",
    "\n",
    "- Spark tries to make Big Data easier (and faster), including moving into production\n",
    "- What is there is very easy but often you will have to \"Roll Your Own\" (RYO) code\n",
    "- You can limit \"re-inventing the wheel\" with help from the community via Spark Packages\n",
    "- Spark has powerful (but limited) tools for text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
