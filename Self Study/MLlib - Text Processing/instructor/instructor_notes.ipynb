{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "TODOs\n",
    "----\n",
    "\n",
    "- https://rideondata.wordpress.com/2015/06/29/analyzing-wikipedia-text-with-pyspark/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add tf-idf with Grimm fairy tales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!  wget -r -np -nH https://www.cs.cmu.edu/~spok/grimmtmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load documents (one per line).\n",
    "words = (sc\n",
    "         .textFile(\"grimms_fairy_tales.txt\")\n",
    "         .flatMap(lambda line: line.split(\" \"))\n",
    "         .map(lambda x: x.replace(',',' ').replace('.',' ').replace('-',' ').lower())\n",
    "        )\n",
    "words.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hashingTF = HashingTF()\n",
    "tf = hashingTF.transform(words)\n",
    "tf.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.cache()\n",
    "idf = IDF().fit(tf)\n",
    "tfidf = idf.transform(tf)\n",
    "tfidf.take(1) #=> (numFeatures, {term_index, term_frequency})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Agenda\n",
    "----\n",
    "\n",
    "1. Announcements\n",
    "    1. Changes based on survey\n",
    "    2. Plan for the week\n",
    "        1. Who\n",
    "        2. Databrick cloud is not working for streaming.\n",
    "            - Install 1.6 locally today during lab. I'll help you.\n",
    "        3. It is very hard to have the right level of difficulty. I'm trying very hard but I just run of time    \n",
    "2. Review / Q&A\n",
    "3. RAT\n",
    "4. Lecture: MLpipelines & text data\n",
    "4. Lab: text data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Extra Materials\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "Big Data median\n",
    "\n",
    "\n",
    "In addition, RDDs support `sample()` and `sampleByKey()` to build simple and stratified samples of data.\n",
    "\n",
    "This can be very useful for bootstrapping.\n",
    "\n",
    "![](http://maths.nayland.school.nz/Year_13_Maths/3.10_Inference/Images/ScreenShot144.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Classification and Regression\n",
    "-----------------------------\n",
    "\n",
    "For binary classification, MLlib expects the labels 0 and 1. In some texts, –1 and 1 are used instead, but this will lead to incorrect results. For multiclass classification, MLlib expects labels from 0 to C–1, where C is the number of classes.\n",
    "\n",
    "-----------------------------\n",
    "\n",
    "Almost all machine learning objectives are optimized using this update\n",
    "$$w\\leftarrow w-\\alpha\\cdot\\sum_{i=1}^ng(w;x_i,y_i)$$\n",
    "$w$ is a vector of dimension $d$  \n",
    "we’re trying to find the best $w$ via optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Load training data\n",
    "training = sqlContext.read.format(\"libsvm\").load(\"data/mllib/sample_libsvm_data.txt\")\n",
    "\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "\n",
    "# Fit the model\n",
    "lrModel = lr.fit(training)\n",
    "\n",
    "# Print the coefficients and intercept for logistic regression\n",
    "print(\"Coefficients: \" + str(lrModel.coefficients))\n",
    "print(\"Intercept: \" + str(lrModel.intercept))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Logistic Regression Results\n",
    "![](http://a3ab771892fd198a96736e50.javacodegeeks.netdna-cdn.com/wp-content/uploads/2015/05/hadoop_spark_comparison.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
