{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Kafka\n",
    "\n",
    "![](https://s3-us-west-2.amazonaws.com/dsci/6007/assets/kafka-logo.png)\n",
    "\n",
    "What is Kafka?\n",
    "\n",
    "- Kafka is like TiVo for real time messages.\n",
    "- Producers write events.\n",
    "- Consumers process events when they can.\n",
    "\n",
    "What are events?\n",
    "\n",
    "- Events are messages or objects.\n",
    "- Think data structures containing fields.\n",
    "- Like SQL records or CSV records.\n",
    "\n",
    "How long does Kafka store events?\n",
    "\n",
    "- By default, for a week.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "By the end of today's lesson, you will be able to:\n",
    "\n",
    "- Discuss what problem Kafka solves and why it exists\n",
    "- Discuss what problems Kafka does not solve, and for which you need\n",
    "  to integrate it with HBase and Spark Streaming.\n",
    "- Install and launch Kafka. \n",
    "- Implement Kafka to handle heavy message volumes using *partitions*,\n",
    "  *consumer groups*, and *replication*.\n",
    "- Implement Kafka *producers* and *consumers* that can send and\n",
    "  receive messages through Kafka. \n",
    "\n",
    "## Etymology\n",
    "\n",
    "Why is it called Kafka?\n",
    "\n",
    "According to [Jay Kreps](https://news.ycombinator.com/item?id=6878186)\n",
    "\n",
    "- The rationale was that it should be a writer because we were\n",
    "  building a distributed log or journal (a service dedicated to\n",
    "  writing). \n",
    "- The writer needed to be someone that (1) I liked, (2) sounded cool\n",
    "  as a name, (3) was dead (because it would be creepy to use the name\n",
    "  of someone who was alive).\n",
    "\n",
    "## Jay Kreps\n",
    "\n",
    "![](images/kafka-jay-kreps.jpg)\n",
    "\n",
    "Who is Jay Kreps?\n",
    "\n",
    "- Co-founder of <http://confluent.io>. \n",
    "- Architect and co-creator of Apache Kafka.\n",
    "- Author of book [*I Heart Logs*](http://shop.oreilly.com/product/0636920034339.do)\n",
    "\n",
    "## Franz Kafka\n",
    "\n",
    "![](images/kafka-franz.jpg)\n",
    "\n",
    "## History of Kafka\n",
    "\n",
    "Year |Event\n",
    "---- |-----\n",
    "2011 |Open sourced at LinkedIn\n",
    "2012 |Version 0.7 released\n",
    "2012 |Graduated from Apache incubator status\n",
    "2013 |Version 0.8 released\n",
    "2014 |Confluent spins off from LinkedIn with goal to productize Kafka\n",
    "2015 |Version 0.9 released\n",
    "2016 |Version 0.10 released\n",
    "\n",
    "## Programming Languages\n",
    "\n",
    "What language is Kafka written in?\n",
    "\n",
    "- Written in Scala.\n",
    "- API supports Scala, Java, Python.\n",
    "\n",
    "## Why Does Kafka Exist\n",
    "\n",
    "LinkedIn’s motivation for Kafka: \n",
    "\n",
    "- Unified platform for handling all the data feeds.\n",
    " \n",
    "Kafka Features:\n",
    "\n",
    "- High throughput to support high volume event feeds.\n",
    "- Support  processing of these feeds to create new, derived feeds.\n",
    "- Support large data backlogs to handle periodic ingestion from offline systems.\n",
    "- Support low-latency delivery to handle more traditional messaging use cases.\n",
    "- Guarantee fault-tolerance in the presence of machine failures.\n",
    "\n",
    "## Kafka Architecture\n",
    "\n",
    "Image of a 2013 discussion of Kafka's Data Architecture at LinkedIn:\n",
    "\n",
    "![](images/kafka-linkedIn2013.png)\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "- **LinkedIn** \n",
    "    - Processes more than 800 billion events/day via Kafka. \n",
    "    - This is 175 TB of data.\n",
    "    - Primary types of data being processed through Kafka are:\n",
    "        - Metrics: operational telemetry data\n",
    "        - Tracking: everything a LinkedIn.com user does\n",
    "        - Queuing: between LinkedIn apps, e.g. for sending emails\n",
    "- **Netflix** \n",
    "    - Real time monitoring and event processing.\n",
    "- **Twitter** \n",
    "    - As part of their Storm and Heron  data pipelines.\n",
    "- **Spotify**\n",
    "    - Log delivery (from 4h down to 10s), Hadoop.\n",
    "- **Loggly** \n",
    "    - Log collection and processing.\n",
    "- **Mozilla** \n",
    "    - Telemetry data.\n",
    "    - Data that Firefox receives from browsers in the wild.\n",
    "\n",
    "## Why Is Kafka Fast\n",
    "\n",
    "- Fast writes:\n",
    "    - While Kafka persists all data to disk, essentially all writes go\n",
    "      to the page cache of OS, i.e. RAM.\n",
    "    - Configuration hardware specs and OS tuning.\n",
    "- Fast reads:\n",
    "    - Very efficient to transfer data from page cache to a network\n",
    "      socket\n",
    "    - Kafka uses the `sendfile()` system call to transfer data\n",
    "      directly from disk to network, without copying it into multiple\n",
    "      buffers.\n",
    "- Combination of the two = fast Kafka!\n",
    "    - For example, on a Kafka cluster where the consumers are mostly\n",
    "      caught up you will see no read activity on the disks as they\n",
    "      will be serving data entirely from cache.\n",
    "\n",
    "## Kafka and Big Data Systems\n",
    "\n",
    "Why is Kafka used in Big Data systems?\n",
    "\n",
    "- Traditionally Big Data systems were batch-oriented.\n",
    "- There are many benefits to doing analytics in real time. \n",
    "- You can detect fraud, network intrusion, etc. and respond to events immediately.\n",
    "- Storm and Spark Streaming let you process data in real time.\n",
    "\n",
    "Why do we need Kafka with Storm and Spark Streaming?\n",
    "\n",
    "- What happens when your data comes in faster than you can process it?\n",
    "- Kafka buffers data so your processes don't crash.\n",
    "- If systems crash or go offline for maintenance Kafka will let them replay the data.\n",
    "- Kafka stores data for a week by default.\n",
    "\n",
    "## Kafka Analogies\n",
    "\n",
    "- Kafka is like a *Universal Power Supply* for data.\n",
    "- Kafka is like TiVo.\n",
    "- Kafka is like Twitter.\n",
    "\n",
    "<!--\n",
    "\n",
    "## Kafka, Spark, HBase\n",
    "\n",
    "<img src=\"images/kafka-spark-hbase.png\">\n",
    "\n",
    "## Kafka, Spark, HBase\n",
    "\n",
    "How does Kafka relate to HBase and Spark?\n",
    "\n",
    "- Kafka, Spark Streaming, and HBase are frequently used together.\n",
    "- Kafka is the real time data pipeline.\n",
    "- Spark Streaming is the real time data processing engine.\n",
    "- HBase is the real time data storage.\n",
    "- Data comes in through Kafka, is processed by Spark Streaming, and is\n",
    "  then saved to HBase.\n",
    "\n",
    "-->\n",
    "\n",
    "# Kafka Architecture\n",
    "\n",
    "## Producers, Consumers, Brokers\n",
    "\n",
    "<img src=\"images/kafka-producer-broker-consumer.png\">\n",
    "\n",
    "## Producers, Consumers, Brokers\n",
    "\n",
    "What are *producers*, *consumer*, *brokers*?\n",
    "\n",
    "- Kafka producers write messages to brokers.\n",
    "- Kafka brokers buffer and retain messages. By default they retain\n",
    "  messages for a week.\n",
    "- Kafka consumers read messages.\n",
    "- Producers and consumers are external to Kafka.\n",
    "- Brokers are what constitute Kafka.\n",
    "\n",
    "## Producers, Consumers, Brokers\n",
    "\n",
    "Intuitively, what are *producers*, *consumer*, *brokers*?\n",
    "\n",
    "- Think of producers as people posting tweets.\n",
    "- Think of consumers as people reading tweets.\n",
    "- Think of brokers as Twitter itself.\n",
    "\n",
    "## Topics\n",
    "\n",
    "<img src=\"images/kafka-producer-topic-consumer.png\">\n",
    "\n",
    "## Topics\n",
    "\n",
    "What are *topics*?\n",
    "\n",
    "- Topics define message streams.\n",
    "- Producers write to specific topics.\n",
    "- Consumers read from specific topics.\n",
    "- Think of these as your favorite show on TV or as Twitter hashtags.\n",
    "\n",
    "## Partitions\n",
    "\n",
    "<img src=\"images/kafka-topic-partitions.png\">\n",
    "\n",
    "\n",
    "## Partitions\n",
    "\n",
    "What are *partitions*?\n",
    "\n",
    "- A topic is made up of partitions.\n",
    "- Producer write messages into partitions.\n",
    "- Each message goes into a particular partition.\n",
    "- Each partition is on a specific broker.\n",
    "\n",
    "## Pop Quiz: Partitions\n",
    "\n",
    "<details><summary>\n",
    "Can a partition have more data than the disk space on a single broker\n",
    "machine?\n",
    "</summary>\n",
    "1. No. <br>\n",
    "2. A partition must fit on a single machine.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Can a topic have more data than the disk space on a single broker\n",
    "machine?\n",
    "</summary>\n",
    "1. Yes. <br>\n",
    "2. A topic can have multiple partitions.<br>\n",
    "3. With partitions Kafka a topic hold more messages than can fit on a machine.<br>\n",
    "</details>\n",
    "\n",
    "## Partitions and Messages\n",
    "\n",
    "![](images/kafka-anatomy-of-topic.png)\n",
    "\n",
    "## Partitions and Messages\n",
    "\n",
    "How does the producer determine which partition a message belongs to?\n",
    "\n",
    "- Default partitioning: round-robin.\n",
    "- Key partitioning: each message has a key and is partitioned by its\n",
    "  key.\n",
    "- Custom partitioner: a custom partitioner class has an opportunity to\n",
    "  look at the key and message and determine which partition to send\n",
    "  the message to.\n",
    "\n",
    "## Pop Quiz: Partitioners\n",
    "\n",
    "<details><summary>\n",
    "If my topic has 5 partitions, will I have 5 copies of every message?\n",
    "</summary>\n",
    "1. No.<br>\n",
    "2. Each message is written in a particular partition.<br>\n",
    "</details>\n",
    "\n",
    "## Partitions\n",
    "\n",
    "Why do partitions exist?\n",
    "\n",
    "- Partitions make the data more parallel.\n",
    "- Different consumers can consume partitions in parallel.\n",
    "\n",
    "## Partitions, Consumers, Consumer Groups\n",
    "\n",
    "![](images/kafka-consumergroup.png)\n",
    "\n",
    "## Partitions, Consumers, Consumer Groups\n",
    "\n",
    "How do collections of consumers coordinate their consuming? And what\n",
    "are *consumer groups*?\n",
    "\n",
    "- Consumer groups are collections of consumers that consume a topic\n",
    "  together.\n",
    "- The partitions are divided between the consumers in a *consumer\n",
    "  group*.\n",
    "- Each consumer group will see all the messages in the topic.\n",
    "- Each consumer in a consumer group will see all the messages in the\n",
    "  partitions assigned to it.\n",
    "- Each partition has exactly one consumer in each consumer group.\n",
    "\n",
    "## Pop Quiz: Consumer Groups\n",
    "\n",
    "<details><summary>\n",
    "Suppose you order two pizza for 4 people. One is meat the other is\n",
    "vegetarian. What Kafka concepts do slices, pizzas, and people\n",
    "correspond to?\n",
    "</summary>\n",
    "1. The slices are messages.<br>\n",
    "2. The pizzas are partitions.<br>\n",
    "3. The two vegetarians are consumers who are assigned the vegetarian\n",
    "consumer group.<br>\n",
    "4. The two non-vegetarians are consumers who are assigned the meat\n",
    "consumer group.<br>\n",
    "</details>\n",
    "\n",
    "## Pop Quiz: Consumer Groups\n",
    "\n",
    "<details><summary>\n",
    "If two consumer groups are consuming from the same topic, will each\n",
    "one of them get all the messages in the topic?\n",
    "</summary>\n",
    "1. Yes.<br>\n",
    "2. Every consumer group sees all the messages.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 2 consumers and the topic has 8 partitions,\n",
    "how many partitions will each consumer get?\n",
    "</summary>\n",
    "Each consumer will get 4 of the partitions.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 8 consumers and the topic has 2 partitions,\n",
    "how many partitions will each consumer get?\n",
    "</summary>\n",
    "1. Two of the 8 consumers will get a partition each.<br>\n",
    "2. The other 6 consumers will not get any partitions.<br>\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "If a consumer group has 2 consumers and the topic has 2 partitions,\n",
    "will each consumer see all the messages? \n",
    "</summary>\n",
    "1. No.<br>\n",
    "2. Each consumer will only see the topics in its partition.<br>\n",
    "</details>\n",
    "\n",
    "## Partition Offset\n",
    "\n",
    "How does a consumer keep track of where it is in the partition?\n",
    "\n",
    "- Messages in partitions are assigned a sequential id called the\n",
    "  _offset_ that uniquely identifies each message within the partition.\n",
    "- Consumers track their position in partitions through tuples made up\n",
    "  of `(topic, partition, offset)`.\n",
    "\n",
    "## Replicas\n",
    "\n",
    "<img src=\"images/kafka-partition-replicas.png\">\n",
    "\n",
    "## Replicas\n",
    "\n",
    "What happens if a broker fails? Do we lose all the messages on the\n",
    "partitions on that broker? \n",
    "\n",
    "- To prevent losing the partitions, partitions are replicated across\n",
    "  brokers.\n",
    "- The identical copies of a partition are called *replicas*.\n",
    "\n",
    "## Leaders and Followers\n",
    "\n",
    "How does Kafka determine which replica to read from and write to?\n",
    "\n",
    "- The replicas are pure backup.\n",
    "- They are never read from or written to.\n",
    "- The brokers holding the partitions for a topic elect a leader.\n",
    "- The leader receives all the read and write requests.\n",
    "- The followers are plain consumers and try to keep up.\n",
    "\n",
    "## Elections\n",
    "\n",
    "What happens if a leader dies?\n",
    "\n",
    "- The followers who are in sync with the leader nominate themselves\n",
    "  for the office of the leader.\n",
    "- If no one is in sync then the follower with the most messages\n",
    "  becomes the leader.\n",
    "\n",
    "## Pop Quiz: Leadership\n",
    "\n",
    "<details><summary>\n",
    "Is there one master broker? Or are there leaders per partition?\n",
    "</summary>\n",
    "\n",
    "- There is no master broker.\n",
    "- There are leaders per partition.\n",
    "- Leadership coordination happens through Zookeeper.\n",
    "</details>\n",
    "\n",
    "\n",
    "## Pop Quiz: Replication\n",
    "\n",
    "<details><summary>\n",
    "If a topic has 8 partitions, and each partition is replicated 3 times,\n",
    "how many copies of each message in this topic exist in Kafka?\n",
    "</summary>\n",
    "3 copies.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "\n",
    "# Installing Kafka\n",
    "\n",
    "## Downloading\n",
    "\n",
    "How can I download Kafka?\n",
    "\n",
    "- Go to <http://kafka.apache.org/downloads.html>\n",
    "\n",
    "- Download the version labeled `Scala 2.10 - kafka_2.10-0.8.2.1.tgz (asc, md5)`\n",
    "\n",
    "- This is the version Kafka's\n",
    "  [quickstart](http://kafka.apache.org/documentation.html#quickstart)\n",
    "  recommends.\n",
    "\n",
    "- Uncompress and untar the binary. Then switch to expanded directory.\n",
    "\n",
    "```bash\n",
    "tar xvzf kafka_2.10-0.8.2.1.tgz \n",
    "cd kafka_2.10-0.8.2.1\n",
    "```\n",
    "\n",
    "[kafka-quickstart]: \n",
    "\n",
    "What is the latest version? Are we using an older version?\n",
    "\n",
    "- According to Kafka's documentation: *Kafka 0.9.0.0 is the latest\n",
    "  release. The current stable version is 0.8.2.2.*\n",
    "- Spark's integration with Kafka uses 0.8.2.1, compiled with Scala\n",
    "  2.10.4. \n",
    "- This is what ships with Spark and works out of the box.\n",
    "\n",
    "## Launching\n",
    "\n",
    "What are the steps for launching Kafka?\n",
    "\n",
    "- Start Zookeeper unless you already have it running. \n",
    "\n",
    "```bash\n",
    "./bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "```\n",
    "\n",
    "- Start the Kafka server which will launch the brokers.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-server-start.sh config/server.properties\n",
    "```\n",
    "\n",
    "## Kick Tires\n",
    "\n",
    "Let's take Kafka out for a spin. Kafka has command line producers and\n",
    "consumers that you can use to test it.\n",
    "\n",
    "- Create a topic.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic test \\\n",
    "  --create \\\n",
    "  --replication-factor 1 \\\n",
    "  --partitions 1\n",
    "```\n",
    "\n",
    "- See that the topic was created.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh  \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --list\n",
    "```\n",
    "\n",
    "- Start the command line consumer.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-console-consumer.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic test \\\n",
    "  --from-beginning\n",
    "```\n",
    "\n",
    "- Publish hello world message through command line producer.\n",
    "\n",
    "```bash\n",
    "echo 'hello world' | \\\n",
    "  ./bin/kafka-console-producer.sh \\\n",
    "    --broker-list localhost:9092 \\\n",
    "    --topic test \n",
    "```\n",
    "\n",
    "- Ignore warning `WARN Property topic is not valid\n",
    "  (kafka.utils.VerifiableProperties)`. \n",
    "\n",
    "- This is a [known bug](https://issues.apache.org/jira/browse/KAFKA-1711).\n",
    "\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help\n",
    "\n",
    "What other options do these command line tools have?\n",
    "\n",
    "- For help on these command line tools run them without any arguments.\n",
    "- They will list out all their options and what effect they have.\n",
    "\n",
    "\n",
    "# Kafka Commands\n",
    "\n",
    "## Creating Topics\n",
    "\n",
    "How can I create a topic?\n",
    "\n",
    "You can create topics using the command line `kafka-topics.sh` tool.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --create \\\n",
    "  --replication-factor 1 \\\n",
    "  --partitions 1\n",
    "```\n",
    "\n",
    "## Changing Topic Configuration\n",
    "\n",
    "How can I change the configuration value per topic?\n",
    "\n",
    "You can change the maximum message size from `1000*1000` to `64000`.\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --alter \\\n",
    "  --config max.message.bytes=64000\n",
    "```\n",
    "\n",
    "## Describing Topic\n",
    "\n",
    "How can I find the configuration settings of a topic?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --describe\n",
    "```\n",
    "\n",
    "## Reverting Configuration Overrides\n",
    "\n",
    "How can I revert a configuration override?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --alter \\\n",
    "  --deleteConfig max.message.bytes\n",
    "```\n",
    "\n",
    "## Deleting Topics\n",
    "\n",
    "How can I delete a topic?\n",
    "\n",
    "```bash\n",
    "./bin/kafka-topics.sh \\\n",
    "  --zookeeper localhost:2181 \\\n",
    "  --topic my-topic \\\n",
    "  --delete \n",
    "```\n",
    "\n",
    "Note: `--delete` was\n",
    "[broken](https://issues.apache.org/jira/browse/KAFKA-1397) until\n",
    "version `0.8.2.0`.\n",
    "\n",
    "# Kafka API\n",
    "\n",
    "## Sending and Receiving Strings\n",
    "\n",
    "How can I use Kafka to send and receive strings?\n",
    "\n",
    "```python\n",
    "import threading, logging, time\n",
    "\n",
    "from kafka.client   import KafkaClient\n",
    "from kafka.consumer import SimpleConsumer\n",
    "from kafka.producer import SimpleProducer\n",
    "\n",
    "class Producer(threading.Thread):\n",
    "    '''Sends messages to Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        client = KafkaClient(\"localhost:9092\")\n",
    "        producer = SimpleProducer(client)\n",
    "        while True:\n",
    "            producer.send_messages('my-topic', \"test\")\n",
    "            producer.send_messages('my-topic', \"Hello, world!\")\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "class Consumer(threading.Thread):\n",
    "    '''Consumes messages from Kafka topic.'''\n",
    "    daemon = True\n",
    "    def run(self):\n",
    "        client = KafkaClient(\"localhost:9092\")\n",
    "        consumer = SimpleConsumer(client, \"test-group\", \"my-topic\")\n",
    "        for message in consumer:\n",
    "            print(message)\n",
    "\n",
    "def main():\n",
    "    '''Starts producer and consumer threads.'''\n",
    "    threads = [ Producer(), Consumer() ]\n",
    "    for t in threads:\n",
    "        t.start()\n",
    "    time.sleep(5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(\n",
    "        format='%(asctime)s.%(name)s:%(message)s',\n",
    "        level=logging.DEBUG)\n",
    "    main()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avro\n",
    "\n",
    "## The need for an enforceable schema\n",
    "\n",
    "Suppose you chose to represent Tom’s age using JSON:\n",
    "\n",
    "```json\n",
    "{\"id\": 3, \"field\":\"age\", \"value\":28, \"timestamp\":1333589484}\n",
    "```\n",
    "\n",
    "There’s no way to ensure that all subsequent facts will follow the same format.\n",
    "\n",
    "```json\n",
    "{\"name\": \"Alice\", \"field\":\"age\", \"value\":25, \"timestamp\":\"2012/03/29 08:12:24\"}\n",
    "{\"id\":2, \"field\":\"age\", \"value\":36}\n",
    "```\n",
    "Both of these examples are valid JSON, but they have inconsistent formats or\n",
    "missing data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://avro.apache.org/images/avro-logo.png)\n",
    "\n",
    "## Avro Advantages\n",
    "\n",
    "Avro stores both the data definition and the data together in one message or file making it easy for programs to dynamically understand the information. \n",
    "\n",
    "1. Store data defintion (data types and protocols) in JSON \n",
    "2. Store the data itself compact and efficient binary format\n",
    "\n",
    "What are the advantages of Avro?\n",
    "\n",
    "- Schema evolution\n",
    "- Untagged data\n",
    "- Dynamic typing\n",
    "- High performance\n",
    "\n",
    "## Schema evolution\n",
    "\n",
    "- Avro requires schemas when data is written or read. \n",
    "- You can use different schemas for serialization and deserialization.\n",
    "- Avro handles the missing/extra/modified fields."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro vs Parquet Use Cases\n",
    "\n",
    "<details><summary>\n",
    "You have built a data pipeline. The data gets progressively more\n",
    "structured as it gets processed. Where should you use Avro and where\n",
    "should you use Parquet?\n",
    "</summary>\n",
    "\n",
    "- Use Avro when the data is unstructured, earlier in the pipeline.\n",
    "- Use Parquet when the data is structured and narrow.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka and Avro\n",
    "\n",
    "<img src=\"images/kafka-avro.png\">\n",
    "\n",
    "Why is Avro relevant to Kafka?\n",
    "\n",
    "- Kafka messages are byte sequences.\n",
    "\n",
    "- Kafka only knows how to handle byte sequences.\n",
    "\n",
    "- To send rich data structures through Kafka you have to serialize\n",
    "  them.\n",
    "\n",
    "- Avro gives you efficient and portable serialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--\n",
    "# Lab\n",
    "\n",
    "1. [Getting Started with Avro in Python](http://avro.apache.org/docs/current/gettingstartedpython.html)\n",
    "    1. Skip the \"Download\" section. Instead run `pip install avro`\n",
    "2. [Kafka Lab](lab.md)\n",
    "-->"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
