{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.mapr.com/sites/default/files/otherpageimages/spark-streaming.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why Streaming?\n",
    "-------------------\n",
    "\n",
    "The world is expecting faster answers...\n",
    "<img src=\"images/realtime.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the end of this session, you should be able to:\n",
    "-----\n",
    "\n",
    "- Compare and contrast batch and streaming processing\n",
    "- Describe and list common stream processing frameworks\n",
    "- Draw the Spark Streaming framework\n",
    "- Define a Dstream and perform common operations on it\n",
    "- Write PySpark code for stream processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "From Batch to \"Real-time\"\n",
    "---\n",
    "\n",
    "Spark, like MapReduce, was designed to process data as a batch job.\n",
    "\n",
    "Nightly batch jobs process large amounts of data and generate insights.\n",
    "\n",
    "What if we want to react immediately instead of wait 24 hours.\n",
    "\n",
    "Streaming solves this problem - It lets you process data immediately in near-realtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "What is an example streaming application?\n",
    "-----\n",
    "\n",
    "![](http://www.h2htech.com/wp-content/uploads/2015/09/CyberSecurity.jpg)\n",
    "\n",
    "Suppose you have an intrusion detection system.\n",
    "\n",
    "You process log files to determine if the system is under attack.\n",
    "\n",
    "Batch processing will take 24 hours to raise an intrusion alert.\n",
    "\n",
    "Spark Streaming can detect an intrustion in minutes or even seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "Streaming Frameworks\n",
    "----\n",
    "\n",
    "1. Minibatch - Same as Batch, only very small and hopefully fast\n",
    "2. \"True\" Streaming - Tuple-by-tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Stream Processing Frameworks\n",
    "-----\n",
    "\n",
    "1. [Apache Storm](http://storm.apache.org/)\n",
    "2. [Twitter's Heron](Streaming Frameworks)\n",
    "3. Spark Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How \"Realtime\" is Spark?  Spark Streaming vs Storm\n",
    "------------------------\n",
    "\n",
    "How does Spark Streaming compare with Storm?\n",
    "\n",
    "- Storm is another system for realtime processing of events.\n",
    "\n",
    "- Here is a comparison of Storm and Spark Streaming.\n",
    "\n",
    "Comparison           |Winner     |Spark Streaming      |Storm\n",
    "----------           |------     |---------------      |-----\n",
    "Processing Model     |  -        |Mini batches         |Record-at-a-time\n",
    "Latency              |Storm      |Few seconds          |Sub-second\n",
    "Fault tolerance      |Spark      |Exactly once         |At least once (may be duplicates)\n",
    "Batch integration    |Spark      |Spark                |Requires different framework\n",
    "API                  |Spark      |Simpler              |Complex\n",
    "Production use       |Storm      |2013                 |2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Spark Streaming\n",
    "----\n",
    "\n",
    "1. Features\n",
    "2. Framework\n",
    "3. Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Micro-Batch Concept\n",
    "-------------------\n",
    "\n",
    "How does Spark Streaming work?\n",
    "\n",
    "- Events are grouped into micro batched RDDs.\n",
    "\n",
    "- Each RDD contains events from the last few seconds.\n",
    "\n",
    "- Incoming event stream is turned into RDD stream.\n",
    "\n",
    "- These micro batched RDDs are joined with existing data to raise alerts.\n",
    "\n",
    "Spark Streaming RDDs\n",
    "--------------------\n",
    "\n",
    "How does Spark Streaming integrate with Spark?\n",
    "\n",
    "- Spark Streaming converts incoming events into micro batched RDDs.\n",
    "\n",
    "- These are then processed by the regular Spark APIs.\n",
    "\n",
    "<img src=\"images/streaming-arch.png\">\n",
    "\n",
    "<img src=\"images/streaming-flow-micro-batches.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What could be a downside of minibatch?\n",
    "</summary>\n",
    "Sometimes the processing takes longer than batch window, then the system becomes overwhelmed.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Stack\n",
    "-----------\n",
    "\n",
    "How does Spark Streaming fit into the rest of Spark?\n",
    "\n",
    "- Spark Streaming is a subsystem of Spark.\n",
    "\n",
    "- Spark Streaming enables handling realtime events.\n",
    "\n",
    "<img src=\"images/spark-stack.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark Streaming Big Picture\n",
    "---------------------------\n",
    "\n",
    "- Spark Streaming can consume events from multiple sources.\n",
    "\n",
    "- These are processed and written out to HDFS, databases, and other\n",
    "  systems.\n",
    "\n",
    "<img src=\"images/streaming-input-output-components.png\">\n",
    "\n",
    "\n",
    "DStream Concept\n",
    "---------------\n",
    "\n",
    "- A DStream is a stream of RDDs.\n",
    "\n",
    "- Think of a DStream as an infinite sequence of RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-as-rdds.png\">\n",
    "\n",
    "- The incoming events are batched together into RDDs.\n",
    "\n",
    "<img src=\"images/streaming-dstream-time-i.png\">\n",
    "\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "What happens to an event that is half in batch `time=1` and half in\n",
    "batch `time=2`? Which batch does it go to?\n",
    "</summary>\n",
    "1. It goes to batch `time=2`.<br>\n",
    "2. Incomplete events are meaningless.<br>\n",
    "3. RDDs are formed from fully-formed events.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "Spark Streaming Demo \n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Stop any already running context\n",
    "# sc.stop() \n",
    "\n",
    "# Start a new Spark context\n",
    "sc = SparkContext(\"local[*]\", \"MyFirstStream\") \n",
    "\n",
    "# Create a Spark Streaming Context with batch interval of 1 second\n",
    "ssc = StreamingContext(sc, batchDuration=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "data = [[5, 5], [4, 4], [3, 3], [2, 2], [1, 1], [0, 0]] \n",
    "\n",
    "# Put sample data in a RDD queue\n",
    "rdd_queue = [sc.parallelize(_, 1) for _ in data] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
       " ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
       " ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
       " ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
       " ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423,\n",
       " ParallelCollectionRDD[5] at parallelize at PythonRDD.scala:423]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add our queue to the stream\n",
    "input_stream = ssc.queueStream(rdd_queue) \n",
    "\n",
    "# Just print the contents of each RDD as it streams by\n",
    "# Remeber in Python 2 print is a not grown-up function (it is a statement)\n",
    "from __future__ import print_function \n",
    "\n",
    "# input_stream.foreachRDD(lambda rdd: print(rdd.collect()))\n",
    "\n",
    "input_stream.map(lambda x: x + 1).pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:39\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:40\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:41\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:42\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:43\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:44\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:45\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:46\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:00:47\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Start stream processing\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stop stream processing\n",
    "ssc.stop(stopSparkContext=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Common Spark Streaming data sources\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- The `StreamingContext` is stored in `ssc`.\n",
    "\n",
    "- `ssc.socketTextStream` creates a `DStream`.\n",
    "\n",
    "- DStreams transformations like `flatMap`, `map`, `reduceByKey` \n",
    "  create new DStreams.\n",
    "\n",
    "- DStreams output operations like `pprint` are like RDD actions.\n",
    "\n",
    "- Except DStream output operations do not cause execution.\n",
    "\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "When you execute `pprint` on a DStream will anything be printed?\n",
    "</summary>\n",
    "1. Nothing is printed.<br>\n",
    "2. The printing happens when we call `ssc.start()` and when data flows in.\n",
    "</details>\n",
    "\n",
    "RDDs vs DStreams\n",
    "----------------\n",
    "\n",
    "How are DStream different from RDDs?\n",
    "\n",
    "- DStream transformations and output operations define an assembly line.\n",
    "  \n",
    "- Nothing happens until data comes in.\n",
    "\n",
    "- When data comes in DStream output operations trigger the execution\n",
    "  of DStream transformations.\n",
    "\n",
    "<img src=\"images/donuts.jpg\">\n",
    "\n",
    "Transformations and Output Operations\n",
    "=====================================\n",
    "\n",
    "DStream Transformations\n",
    "-----------------------\n",
    "\n",
    "How are DStream transformations different from RDD transformations?\n",
    "\n",
    "- DStream transformations define what will happen to RDDs when they\n",
    "  arrive.\n",
    "  \n",
    "- DStream transformations produce new DStreams that will contain \n",
    "  transformed RDDs.\n",
    "\n",
    "- Nothing happens until data arrives.\n",
    "\n",
    "<img src=\"images/streaming-dstream-ops.png\">\n",
    "\n",
    "Transforming DStreams\n",
    "---------------------\n",
    "\n",
    "Transformation                                 |For Each Incoming RDD\n",
    "--------------                                 |---------------------\n",
    "`ds.map(lambda line: line.upper())`            |Uppercase `line` \n",
    "`ds.flatMap(lambda line: line.split())`        |Split `line` into words\n",
    "`ds.filter(lambda line: line.strip() != '')`   |Exclude `line` if it is empty\n",
    "`ds.repartition(10)`                           |Repartition RDD into 10 partitions\n",
    "`ds.reduceByKey(lambda v1,v2: v1+v2)`          |For each key sum values \n",
    "`ds.groupByKey()`                              |For each key group values into iterable\n",
    "\n",
    "Generic Transformations\n",
    "-----------------------\n",
    "\n",
    "How can I apply an arbitrary transformation on the incoming RDDs?\n",
    "\n",
    "- DStreams have some but not all of the transformations as RDDs.\n",
    "\n",
    "- For example, `sortByKey()` is not supported on DStreams.\n",
    "\n",
    "- Instead DStreams provide `transform()` \n",
    "\n",
    "- `transform()` lets you translate any RDD transformation to DStreams.\n",
    "\n",
    "- These two have the same effect.\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "```python\n",
    "ds.flatMap(lambda line: line.split())\n",
    "```\n",
    "\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "How can you write `sortByKey()` for DStreams?\n",
    "</summary>\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.sortByKey())\n",
    "```\n",
    "</details>\n",
    "\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "Consider this code:\n",
    "\n",
    "```python\n",
    "ds.transform(lambda rdd: rdd.flatMap(lambda line: line.split()))\n",
    "```\n",
    "\n",
    "<details><summary>\n",
    "Where does `lambda line: ...` execute? \n",
    "</summary>\n",
    "On the executors.\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary>\n",
    "Where does `lambda rdd: ...` execute? \n",
    "</summary>\n",
    "On the driver.\n",
    "</details>\n",
    "\n",
    "\n",
    "DStream Output Operations\n",
    "-------------------------\n",
    "\n",
    "Expression                                     |Meaning\n",
    "----------                                     |-------\n",
    "`ds.foreachRDD(lambda rdd: func(rdd.first()))` |Call `func()` on `first()` of each incoming RDD\n",
    "`ds.pprint(num=10)`                            |Print first 10 elements of each incoming RDD\n",
    "`ds.saveAsTextFiles('foo',suffix=None)`        |Save each incoming RDD's partitions to disk\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "- These output operations only execute when RDDs start arriving.\n",
    "\n",
    "- `foreachRDD` is a generic output operation.\n",
    "\n",
    "- `foreachRDD` lets you define arbitrary output operations on incoming RDDs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Print the count of incoming RDDs.\n",
    "</summary>\n",
    "```python\n",
    "# Enable print as a function\n",
    "from __future__ import print_function\n",
    "\n",
    "# Define the output operation\n",
    "ds.foreachRDD(lambda rdd: print(rdd.count()))\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Where will the lambda inside the `foreachRDD` execute?\n",
    "</summary>\n",
    "1. It will execute on the driver.<br>\n",
    "2. This is because RDDs are defined on the driver, not on the executors.<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Streaming Apps Using QueueStream\n",
    "----------------------------------------\n",
    "\n",
    "Manually testing apps using `nc` is quite tedious. Is there an\n",
    "easier more automatable way to do this?\n",
    "\n",
    "- *Queue streams* enable you to create preprogrammed streams perfect\n",
    "  for automated testing and test-driven development.\n",
    "\n",
    "Counting Event Types\n",
    "--------------------\n",
    "\n",
    "Count how many events of different types are in incoming stream in\n",
    "each micro-batch.\n",
    "\n",
    "- Here is the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 0, 1, 2, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(3) * 2 + range(5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_queue_stream.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_queue_stream.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "import random\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "print 'Initializing event_rdd_queue'\n",
    "## This is my test data\n",
    "event_rdd_queue = []\n",
    "for i in xrange(5):\n",
    "    events = range(5) * 10 + range(10) * 10\n",
    "    event_rdd = ssc.sparkContext.parallelize(events)\n",
    "    event_rdd_queue.append(event_rdd)\n",
    "pprint(event_rdd_queue)\n",
    "\n",
    "print 'Building DStream pipeline'\n",
    "ds = ssc\\\n",
    "    .queueStream(event_rdd_queue) \\\n",
    "    .map(lambda event: (event, 1)) \\\n",
    "    .reduceByKey(lambda v1,v2: v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ssc\n",
      "Initializing event_rdd_queue\n",
      "[ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423]\n",
      "Building DStream pipeline\n",
      "Starting ssc\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:16\n",
      "-------------------------------------------\n",
      "(0, 20)\n",
      "(8, 10)\n",
      "(4, 20)\n",
      "(1, 20)\n",
      "(5, 10)\n",
      "(9, 10)\n",
      "(2, 20)\n",
      "(6, 10)\n",
      "(3, 20)\n",
      "(7, 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:17\n",
      "-------------------------------------------\n",
      "(0, 20)\n",
      "(8, 10)\n",
      "(4, 20)\n",
      "(1, 20)\n",
      "(5, 10)\n",
      "(9, 10)\n",
      "(2, 20)\n",
      "(6, 10)\n",
      "(3, 20)\n",
      "(7, 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:18\n",
      "-------------------------------------------\n",
      "(0, 20)\n",
      "(8, 10)\n",
      "(4, 20)\n",
      "(1, 20)\n",
      "(5, 10)\n",
      "(9, 10)\n",
      "(2, 20)\n",
      "(6, 10)\n",
      "(3, 20)\n",
      "(7, 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:19\n",
      "-------------------------------------------\n",
      "(0, 20)\n",
      "(8, 10)\n",
      "(4, 20)\n",
      "(1, 20)\n",
      "(5, 10)\n",
      "(9, 10)\n",
      "(2, 20)\n",
      "(6, 10)\n",
      "(3, 20)\n",
      "(7, 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:20\n",
      "-------------------------------------------\n",
      "(0, 20)\n",
      "(8, 10)\n",
      "(4, 20)\n",
      "(1, 20)\n",
      "(5, 10)\n",
      "(9, 10)\n",
      "(2, 20)\n",
      "(6, 10)\n",
      "(3, 20)\n",
      "(7, 10)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:21\n",
      "-------------------------------------------\n",
      "\n",
      "Stopping ssc\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:22\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 15:16:23\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 4]\r",
      "[Stage 0:>                                                          (0 + 4) / 4]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_queue_stream.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating RDD\n",
    "===============\n",
    "\n",
    "Merging DStreams\n",
    "----------------\n",
    "\n",
    "Transformation      |Effect\n",
    "--------------      |------\n",
    "`ds1.union(ds2)`    |Combine RDD in `ds1` with RDD in same batch in `ds2`\n",
    "`ds1.join(ds2)`     |Join RDD in `ds1` with RDD in same batch in `ds2`\n",
    "\n",
    "Note\n",
    "----\n",
    "\n",
    "- For `union` or `join` the DStreams must have identical batch\n",
    "  durations.\n",
    "\n",
    "- The batches are matched up based on timestamps.\n",
    "\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "How can I process multiple RDDs within a window of time?\n",
    "\n",
    "```python\n",
    "ds2 = ds1.window(windowDuration=30, slideDuration=10)\n",
    "```\n",
    "\n",
    "- Batches RDDs into 30-second windows \n",
    "\n",
    "- Produces new window every 10 seconds\n",
    "\n",
    "<img src=\"images/streaming-dstream-window.png\">\n",
    "\n",
    "Windowing Operations\n",
    "--------------------\n",
    "\n",
    "Calculate the average of a series of heads and tails using a\n",
    "window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_window.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_window.py\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "print 'Initializing rdd_queue'\n",
    "rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    rdd_data = xrange(1000)\n",
    "    rdd = ssc.sparkContext.parallelize(rdd_data)\n",
    "    rdd_queue.append(rdd)\n",
    "pprint(rdd_queue)\n",
    "\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(rdd_queue)\\\n",
    "    .map(lambda x: (x % 10, 1))\\\n",
    "    .window(windowDuration=4,slideDuration=2)\\\n",
    "    .reduceByKey(lambda v1,v2:v1+v2)\n",
    "ds.pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "time.sleep(20)\n",
    "\n",
    "print 'Stopping ssc'\n",
    "ssc.stop(stopSparkContext=True, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ssc\n",
      "Initializing rdd_queue\n",
      "[PythonRDD[5] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[6] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[7] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[8] at RDD at PythonRDD.scala:43,\n",
      " PythonRDD[9] at RDD at PythonRDD.scala:43]\n",
      "Creating queue stream\n",
      "Starting ssc\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:03\n",
      "-------------------------------------------\n",
      "(0, 200)\n",
      "(8, 200)\n",
      "(4, 200)\n",
      "(1, 200)\n",
      "(5, 200)\n",
      "(9, 200)\n",
      "(2, 200)\n",
      "(6, 200)\n",
      "(3, 200)\n",
      "(7, 200)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:05\n",
      "-------------------------------------------\n",
      "(0, 400)\n",
      "(8, 400)\n",
      "(4, 400)\n",
      "(1, 400)\n",
      "(5, 400)\n",
      "(9, 400)\n",
      "(2, 400)\n",
      "(6, 400)\n",
      "(3, 400)\n",
      "(7, 400)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:07\n",
      "-------------------------------------------\n",
      "(0, 300)\n",
      "(8, 300)\n",
      "(4, 300)\n",
      "(1, 300)\n",
      "(5, 300)\n",
      "(9, 300)\n",
      "(2, 300)\n",
      "(6, 300)\n",
      "(3, 300)\n",
      "(7, 300)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:09\n",
      "-------------------------------------------\n",
      "(0, 100)\n",
      "(8, 100)\n",
      "(4, 100)\n",
      "(1, 100)\n",
      "(5, 100)\n",
      "(9, 100)\n",
      "(2, 100)\n",
      "(6, 100)\n",
      "(3, 100)\n",
      "(7, 100)\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:11\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:13\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:15\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:17\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:19\n",
      "-------------------------------------------\n",
      "\n",
      "Stopping ssc\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:21\n",
      "-------------------------------------------\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-02-18 16:29:23\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16/02/18 16:28:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/18 16:28:59 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "\r",
      "[Stage 0:>                                                                                   (0 + 0) / 8]\r",
      "[Stage 0:>                                                                                   (0 + 4) / 8]\r",
      "[Stage 0:==========>                                                                         (1 + 4) / 8]\r",
      "                                                                                                         \r"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_window.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Summary\n",
    "----\n",
    "- Streaming is the new black for data processing\n",
    "- It is a different way of processing. That requires its own idioms and logic\n",
    "- Spark Streaming is a mini-batch system based on the DStream abstraction\n",
    "- DStreams have "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
