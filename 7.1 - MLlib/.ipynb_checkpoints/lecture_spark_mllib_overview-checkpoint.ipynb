{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://kodcu.com/wp/wp-content/uploads/2014/06/mllib.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://img.scoop.it/_VP0qLV-jYbp2cFF4H4k4jl72eJkfbmt4t8yenImKBVvK0kTmF0xjctABnaLJIm9\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/b7FNjKdGXEFos/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Describe Spark's machine learning framework - MLlib\n",
    "- Fit K-means clustering\n",
    "- Apply Collaborative Filtering to make recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MLlib Overview\n",
    "---\n",
    "\n",
    "MLlib is a Spark subproject providing machine learning primitives that are \"production\" ready.\n",
    "\n",
    "Started by AMPLab at UC Berkeley\n",
    "\n",
    "Widespread adoption and support - 80+ contributors from various organization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "MLlib is not always needed\n",
    "----\n",
    "\n",
    "__Remember__: You do not _need_ to use MLlib to do large scale machine learning\n",
    "\n",
    "Depending on the nature of the problem:\n",
    "\n",
    "1. Use a large single-node machine learning library (_i.e._ `scikit-learn`) on big EC2\n",
    "2. Distrubte embarassingly parallel problems as grid search without a framework "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "MLlib Algorithms\n",
    "----\n",
    "\n",
    "- logistic regression and linear support vector machine (SVM)\n",
    "- classification and regression tree\n",
    "- random forest and gradient-boosted trees\n",
    "- recommendation via alternating least squares (ALS)\n",
    "- clustering via k-means, bisecting k-means, Gaussian mixtures (GMM), and power iteration clustering\n",
    "- topic modeling via latent Dirichlet allocation (LDA)\n",
    "- survival analysis via accelerated failure time model\n",
    "- singular value decomposition (SVD) and QR decomposition\n",
    "- principal component analysis (PCA)\n",
    "- linear regression with L1, L2, and elastic-net regularization\n",
    "- isotonic regression\n",
    "- multinomial/binomial naive Bayes\n",
    "- frequent itemset mining via FP-growth and association rules\n",
    "- sequential pattern mining via PrefixSpan\n",
    "- summary statistics and hypothesis testing\n",
    "- feature transformations\n",
    "- model evaluation and hyper-parameter tuning\n",
    "\n",
    "[As of 2016-06-14](http://spark.apache.org/mllib/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Points to Ponder\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "What algorithms are not in Spark? Why do you think that is?\n",
    "</summary>\n",
    "Deep Learning. [They are working on it](http://arxiv.org/abs/1511.06051)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Statistics\n",
    "---\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/7fdcabe25caf35b9001e5ed1ec9f100d16d77d2d/687474703a2f2f7777772e6d61746866756e6e792e636f6d2f696d616765732f6d6174686a6f6b652d686168612d68756d6f722d6d6174682d6d656d652d6a6f6b652d7069632d6d6174686d656d652d66756e6e79706963732d70756e2d7374616e64617264646576696174696f6e2d737461746973746963732d6e6f726d2d6c6f76652e6a7067\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pyspark.mllib.stat import Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-12e05460d4e2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-12e05460d4e2>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Statistics.\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the what is available...\n",
    "Statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([   1.,   10.,  100.]),\n",
       " array([   2.,   20.,  200.]),\n",
       " array([   3.,   30.,  300.])]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a RDD of Vectors\n",
    "mat = sc.parallelize([np.array([1.0, 10.0, 100.0]), np.array([2.0, 20.0, 200.0]), np.array([3.0, 30.0, 300.0])])  \n",
    "mat.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute column summary statistics.\n",
    "summary = Statistics.colStats(mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's look at the what is available...\n",
    "summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean of each column: [   2.   20.  200.]\n",
      "The mean of each clumn: [  1.00000000e+00   1.00000000e+02   1.00000000e+04]\n"
     ]
    }
   ],
   "source": [
    "print(\"The mean of each column: {}\".format(summary.mean()))\n",
    "print(\"The mean of each clumn: {}\".format(summary.variance()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<p><details><summary>Which common statistics are missing from this list? Why?</summary>\n",
    "Median and other quantiles are missing.  \n",
    "<br>\n",
    "Why? Because they are non-associative, thus harder to parrelleize\n",
    "</details></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/mllib-statistics.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "![](images/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/clusters.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/choose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/adjust.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/spark_kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means in Spark 2.0\n",
    "---\n",
    "\n",
    "[Databbrick K-means demo](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6058142077065523/2030838328691810/4338926410488997/latest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.linalg import Vectors\n",
    "# from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data = [(Vectors.dense([0.0, 0.0]),), (Vectors.dense([1.0, 1.0]),),\n",
    "#         (Vectors.dense([9.0, 8.0]),), (Vectors.dense([8.0, 9.0]),)]\n",
    "# df = spark.createDataFrame(data, [\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kmeans = KMeans(k=2, seed=1)\n",
    "# model = kmeans.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# centers = model.clusterCenters()\n",
    "# centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source]( https://people.apache.org/~pwendell/spark-nightly/spark-master-docs/latest/api/python/pyspark.ml.html?highlight=kmeans#pyspark.ml.clustering.KMeans )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "\n",
    "<details><summary>\n",
    "Why is K-means in Spark's MLlib?\n",
    "</summary>\n",
    "K-means requires a complete pass over the data every iteration. Spark is relatively efficient for that specification.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----\n",
    "Recommendations via matrix completion\n",
    "----\n",
    "\n",
    "![](images/recommend.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collaborative Filtering\n",
    "---\n",
    "\n",
    "Goal: predict usersâ€™ movie ratings based on past ratings of other movies\n",
    "\n",
    "$$\n",
    "R = \\left( \\begin{array}{ccccccc}\n",
    "1 & ? & ? & 4 & 5 & ? & 3 \\\\\n",
    "? & ? & 3 & 5 & ? & ? & 3 \\\\\n",
    "5 & ? & 5 & ? & ? & ? & 1 \\\\\n",
    "4 & ? & ? & ? & ? & 2 & ?\\end{array} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<details><summary>\n",
    "What are the challenges of large scale Collaborative Filtering?\n",
    "</summary>\n",
    "Challenges: <br>\n",
    "- Defining similarity that scales <br>\n",
    "- Dimensionality (Millions of Users / Items) <br>\n",
    "- Sparsity, most users has not seen most items <br>\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/solution.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Matrix Factorization](https://databricks-training.s3.amazonaws.com/img/matrix_factorization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternating Least Squares  \n",
    "![ALS](https://databricks.com/wp-content/uploads/2014/07/als-illustration.png)  \n",
    "1. Start with random $U_1$, $V_1$\n",
    "2. Solve for $U_2$ to minimize $||R â€“ U_2V_1^T||$ \n",
    "3. Solve for $V_2$ to minimize $||R â€“ U_2V_2^T||$ \n",
    "4. Repeat until convergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "ALS on Spark\n",
    "---\n",
    "\n",
    "Cache 2 copies of $R$ in memory, one partitioned by rows and one by columns \n",
    "\n",
    "Keep $U~\\&~V$ partitioned in corresponding way \n",
    "\n",
    "Operate on blocks to lower communication\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALS in Spark 2.0\n",
    "---\n",
    "\n",
    "[Databbrick ALS demo](https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/6058142077065523/2030838328691818/4338926410488997/latest.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from pyspark.mllib.recommendation import ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df = spark.createDataFrame([(0, 1, 4.0), (1, 1, 5.0), (1, 2, 4.0), (2, 1, 1.0), (2, 2, 5.0)],\n",
    "#                            [\"user\", \"item\", \"rating\"])\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# als = ALS() \n",
    "# model = als.train(df, rank=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# user = 0\n",
    "# item = 2\n",
    "# model.predict(user, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#module-pyspark.mllib.recommendation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "ALS vs. SVD for recommendation engines\n",
    "---\n",
    "\n",
    "Alternating least squares (ALS) is flexible but less precise.  \"Approximate\" means minimizing some squared-error difference with the input A, but, here you can customize exactly what is considered in the loss function. For example you can ignore missing values (crucial) or weight different values differently.\n",
    "\n",
    "Singular value decomposition (SVD) is a decomposition that gives more guarantees about its factorization. The SVD is relatively more computationally expensive and harder to parallelize. There is also not a good way to deal with missing values or weighting; you need to assume that in your sparse input, missing values are equal to a mean value 0. \n",
    "\n",
    "[Source](https://www.quora.com/What-is-the-difference-between-SVD-and-matrix-factorization-in-context-of-recommendation-engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "- MLlib is Spark's machine learning framework. \n",
    "- A bunch of super smart people are porting even more algorithms to work in distributed, lazy-exectution way.+987\n",
    "- MLlib has the greatest hits of machine learning:\n",
    "    - K-means for clustering\n",
    "    - ALS for Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Bonus Materials\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "MLlib Data Types\n",
    "---\n",
    "\n",
    "[Docs](http://spark.apache.org/docs/latest/mllib-data-types.html)\n",
    "\n",
    "`Local vector`: can be dense or sparse\n",
    "\n",
    "A dense vector is backed by a double array representing its entry values.\n",
    "\n",
    "While a sparse vector is backed by two parallel arrays: indices and values. \n",
    "\n",
    "For example, a vector (1.0, 0.0, 3.0) can be represented in \n",
    "\n",
    "| Dense | Sparse |  \n",
    "|:-------:|:------:|\n",
    "| [1.0, 0.0, 3.0]  | (3, [0, 2], [1.0, 3.0]) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "# Create a SparseVector.\n",
    "sv1 = Vectors.sparse(3, [0, 2], [1.0, 3.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(3, {0: 1.0, 2: 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LabeledPoint`\n",
    "\n",
    "A labeled point is a local vector, either dense or sparse, associated with a label/response for supervised learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Create a labeled point with a positive label and a dense feature vector.\n",
    "pos = LabeledPoint(1.0, [1.0, 0.0, 3.0])\n",
    "\n",
    "# Create a labeled point with a negative label and a sparse feature vector.\n",
    "neg = LabeledPoint(0.0, SparseVector(3, [0, 2], [1.0, 3.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local matrix\n",
    "\n",
    "Take a _wild guess_...\n",
    "\n",
    "> A local matrix has integer-typed row and column indices and double-typed values, stored on a single machine. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributed matrix\n",
    "\n",
    "> A distributed matrix has long-typed row and column indices and double-typed values, stored distributively in one or more RDDs. \n",
    "\n",
    "It is very important to choose the right format to store large and distributed matrices. Converting a distributed matrix to a different format may require a global shuffle, which is quite expensive. Three types of distributed matrices have been implemented so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg.distributed import RowMatrix\n",
    "\n",
    "# Create an RDD of vectors.\n",
    "rows = sc.parallelize([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "# Create a RowMatrix from an RDD of vectors.\n",
    "mat = RowMatrix(rows)\n",
    "\n",
    "# Get its size.\n",
    "m = mat.numRows()  # 4\n",
    "n = mat.numCols()  # 3\n",
    "\n",
    "# Get the rows as an RDD of vectors again.\n",
    "rowsRDD = mat.rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rowsRDD.<tab>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<details><summary>\n",
    "Which data type would you use for random forests?\n",
    "</summary>\n",
    "`LabeledPoint` <br>\n",
    "<br>\n",
    "Random forests is a supervised learning algorithm. It requires labels in order to classify.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:dsci6007]",
   "language": "python",
   "name": "conda-env-dsci6007-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
