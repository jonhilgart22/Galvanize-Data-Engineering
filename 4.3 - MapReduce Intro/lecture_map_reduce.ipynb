{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are we?\n",
    "-----\n",
    "\n",
    "[Let's look at the map](http://insightdataengineering.com/blog/pipeline_map.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce\n",
    "===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/ENtJs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why MapReduce?\n",
    "-------------\n",
    "\n",
    "We have a 100 TB of sales data that looks like this:\n",
    "\n",
    "ID    |Date          |Store  |State |Product   |Amount\n",
    "--    |----          |-----  |----- |-------   |------\n",
    "101   |11/13/2014    |100    |WA    |331       |300.00\n",
    "104   |11/18/2014    |700    |OR    |329       |450.00\n",
    "\n",
    "What If\n",
    "-------\n",
    "\n",
    "What are some of the questions we could answer if we could process this huge data set?\n",
    "\n",
    "- How much revenue did we make by store, state?\n",
    "\n",
    "- How much revenue did we make by product?\n",
    "\n",
    "- How much revenue did we make by week, month, year?\n",
    "\n",
    "\n",
    "Engineering Problem\n",
    "-------------------\n",
    "\n",
    "To answer these questions we have to solve two problems:\n",
    "\n",
    "- Store 100 TB of data\n",
    "\n",
    "- Process 100 TB of data\n",
    "\n",
    "\n",
    "MapReduce 101\n",
    "-----\n",
    "MapReduce is a framework originally developed at Google that allows large scale distributed computing. \n",
    "\n",
    "Apache Hadoop is the open source implementation as the defacto standard for Big Data processing. \n",
    "\n",
    "It scales well to many thousands of nodes and petabytes of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "HDFS and MapReduce are the peanut butter & jelly of Big Data\n",
    "------------------\n",
    "\n",
    "![](http://hadoopilluminated.com/hadoop_illuminated/images/hadoop_coin.png)\n",
    "\n",
    "- HDFS solves the storage problem.\n",
    "\n",
    "- MapReduce works with HDFS to break down queries.\n",
    "    - In the *map* phase the data is processed locally.\n",
    "\n",
    "    - In the *reduce* phase the results of the map phase are consolidated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What problem does Map Reduce solve?\n",
    "-----\n",
    "\n",
    "It's a computing paradigm similar to divide and conquer.\n",
    "\n",
    "If you break up a problem in to smaller sub problems and solve them in parallel, then reduce down to the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "By the end of this session, we will be able to:\n",
    "----\n",
    "- Understand the MapReduce algorithm.\n",
    "\n",
    "- Create MapReduce jobs using MRJob to process large data sets. \n",
    "\n",
    "- Optimize MapReduce jobs so that most processing is done locally via Combiners"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "The MapReduce algorithm\n",
    "---------\n",
    "![](images/MapReduce_overview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does MapReduce work?\n",
    "---------------\n",
    "\n",
    "- The developer (you) provides mapper and reducer code.\n",
    "\n",
    "- The mapper function transforms individual records and attaches a key to each record.\n",
    "\n",
    "- All the records with the same key end up on the same reducer.\n",
    "\n",
    "- For each key the reduce function combines the records with that key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Word Count is \"Hello, World\" of Big Data\n",
    "---\n",
    "\n",
    "![](images/word_count.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MapReduce in Hadoop\n",
    "----\n",
    "\n",
    "<img src=\"images/map-reduce-key-partition.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MapReduce-Data-Flow-of-Word-Count.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How many mappers does each job get?\n",
    "</summary>\n",
    "1. One mapper per block of data.\n",
    "<br>\n",
    "2. Large files get more mappers, small files get fewer.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: How many reducers does each job get?\n",
    "</summary>\n",
    "1. This is configured by the programmer.\n",
    "<br>\n",
    "2. By default each job gets one reducer.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose I want to find out how many sales transactions are in a\n",
    "data set for each state. What key should the mapper output?\n",
    "</summary>\n",
    "1. The mapper should output *state* as the key, and *1* as the value.\n",
    "<br>\n",
    "2. This will ensure that all the records for a specific state end up\n",
    "   on the same reducer.\n",
    "<br>\n",
    "3. The reducer can then add up the *1*s to get the total number of\n",
    "   transactions.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Functional Programming: Put $ into the jar\n",
    "----\n",
    "\n",
    "One thing of note is the idea that map and reduce are both very strong functional programming paradigms.\n",
    "\n",
    "Map takes an input and returns some output. Maps are the composed and piped in to a reduce step for further processing.\n",
    "\n",
    "A higher order operation of map is taking a function and applying a transform on the give input.\n",
    "\n",
    "An easy example of this would be a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input = [1, 2, 3, 4, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Map\n",
    "\n",
    "def map_add_1(input):\n",
    "     return input + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(map_add_1, input) #=> [2, 3, 4, 5, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_two_numbers(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(add_two_numbers, input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Student Activity\n",
    "----\n",
    "\n",
    "<details><summary>\n",
    "Write a map function that counts the length of each word in the following quote: <br>\n",
    "<br>\n",
    "words = \"The concept of global warming was created by and for the Chinese in order to make U.S. manufacturing non-competitive.\".split()\n",
    "\n",
    "</summary>\n",
    "__Solution__: <br>\n",
    "<br>\n",
    "`map(len, words)` <br>\n",
    "<br>\n",
    "`map(lambda word: len(word), words)`\n",
    "\n",
    "<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>\n",
    "Write a reduce function that [1, 2, 3, 4, 5, 6, 7, 8] into 12345678.<br>\n",
    "</summary>\n",
    "__Solution__: <br>\n",
    "<br>\n",
    "reduce(lambda a,d: 10*a+d, [1,2,3,4,5,6,7,8]) <br>\n",
    "or <br>\n",
    "reduce(lambda x,y: str(x)+str(y), [1,2,3,4,5,6,7,8])\n",
    "</details>\n",
    "\n",
    "__Optional__\n",
    "\n",
    "<details><summary>\n",
    "Write a reduce function to flatten a list. <br>\n",
    "Turn: [[1, 2, 3], [4, 5], [6, 7, 8]] into [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "<br>\n",
    "\n",
    "</summary>\n",
    "reduce(lambda x,y: x+y, [[1, 2, 3], [4, 5], [6, 7, 8]]) <br>\n",
    "or <br>\n",
    "reduce(list.__add__, [[1, 2, 3], [4, 5], [6, 7, 8]])\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "MapReduce Using MRJob\n",
    "---\n",
    "\n",
    "![](http://cdn.meme.am/instances/60004133.jpg)\n",
    "\n",
    "Sales Data\n",
    "----------\n",
    "\n",
    "Here is the sales data we are going to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting sales.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile sales.txt\n",
    "#ID    Date           Store   State  Product    Amount\n",
    "101    11/13/2014     100     WA     331        300.00\n",
    "104    11/18/2014     700     OR     329        450.00\n",
    "102    11/15/2014     203     CA     321        200.00\n",
    "106    11/19/2014     202     CA     331        330.00\n",
    "103    11/17/2014     101     WA     373        750.00\n",
    "105    11/19/2014     202     CA     321        200.00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transactions By State\n",
    "---------------------\n",
    "\n",
    "Q: How many transactions were there for each state?\n",
    "\n",
    "- Create the `SaleCount.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SaleCount(MRJob):\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        yield (state, 1)\n",
    "    \n",
    "    def reducer(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/step-0-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/step-0-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/step-0-reducer_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232924.236071\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "¿¿¿¿¿¿¿¿¿¿¿Did that even work???????????????\n",
    "\n",
    "Let's check the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t3\r\n",
      "\"OR\"\t1\r\n",
      "\"WA\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The is what successful MapReduce is like.\n",
    "\n",
    "![](http://bltz.info/wp-content/uploads/2015/11/fml.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose instead of counting transactions by state we want to count\n",
    "transactions by store. What should we change in the code above?\n",
    "</summary>\n",
    "1. Replace `state = field[3]` with `store = field[2]`\n",
    "<br>\n",
    "2. Replace `yield (state, 1)` with `yield (store, 1)`\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Suppose instead of counting transactions we want to find total\n",
    "revenue by state. What should we change in the code above?\n",
    "</summary>\n",
    "1. Add `amount = float(fields[5])` \n",
    "<br>\n",
    "2. Replace `yield (state, 1)` with `yield (state, amount)`\n",
    "</details>\n",
    "\n",
    "Using MapReduce For Statistics\n",
    "------------------------------\n",
    "\n",
    "- Using MapReduce we can calculate statistics for any factors.\n",
    "\n",
    "- Our factor or condition becomes the key.\n",
    "\n",
    "- The parameter that we want to calculate the statistic on becomes\n",
    "  the value.\n",
    "\n",
    "- The reducer contains the logic to apply the statistic.\n",
    "\n",
    "- The statistic can be sum, count, average, stdev, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: What common statistic of central tendency would suck to calculate? Why?\n",
    "</summary>\n",
    "Median <br>\n",
    "<br>\n",
    "Require that 1 reducer is passed the entire range of numbers to determine which is the 'middle' value. <br>\n",
    "<br>\n",
    "[Stackoverflow](http://stackoverflow.com/questions/10109514/computing-median-in-map-reduce)\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using MRJob for Word Count\n",
    "--------------------------\n",
    "\n",
    "Q: Count the frequency of words using MRJob.\n",
    "\n",
    "- Create an input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting input.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile input.txt\n",
    "hello world\n",
    "this is the second line\n",
    "this is the third line\n",
    "hello again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create the `WordCount.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting WordCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile WordCount.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class WordCount(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield word.lower(), 1\n",
    "    \n",
    "    def reducer(self, word, counts): \n",
    "        yield word, sum(counts)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    WordCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/step-0-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/step-0-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/step-0-reducer_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/WordCount.brian.20160202.232924.773338\n"
     ]
    }
   ],
   "source": [
    "!python WordCount.py input.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"again\"\t1\r\n",
      "\"hello\"\t2\r\n",
      "\"is\"\t2\r\n",
      "\"line\"\t2\r\n",
      "\"second\"\t1\r\n",
      "\"the\"\t2\r\n",
      "\"third\"\t1\r\n",
      "\"this\"\t2\r\n",
      "\"world\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "# Check the output:\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Count Notes\n",
    "----------------\n",
    "\n",
    "- WordCount is used as a standard distributed application\n",
    "\n",
    "- For a large number of words it is not solvable on a single machine\n",
    "\n",
    "- A large corpus can require more storage than the disk on a single\n",
    "  machine\n",
    "\n",
    "- A large vocabulary can require more memory than on a single machine.\n",
    "\n",
    "- WordCount generalizes to other counting applications: such as \n",
    "  counting clicks by category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customizing MapReduce\n",
    "---------------------\n",
    "\n",
    "Q: What are the places in the MapReduce pipeline that can be modified\n",
    "using Java classes?\n",
    "\n",
    "<img src=\"images/map-reduce-key-partition.png\">\n",
    "\n",
    "<img src=\"images/map-reduce-phases.png\">\n",
    "\n",
    "Class               |Runs On          |Decides \n",
    "-----               |-------          |------- \n",
    "InputFormat         |Client           |Splits HDFS file to InputSplits\n",
    "InputFormat         |Mapper           |Splits InputSplit to `(key1,value1)`\n",
    "Mapper              |Mapper           |Maps `(key1,value1)` to `(key2,value2)`\n",
    "Partitioner         |Mapper           |Decides which `(key2,value2)` goes to which reducer\n",
    "SortComparator      |Mapper + Reducer |Determines sort order between all the `key2`\n",
    "GroupingComparator  |Reducer          |Groups `(key2,value2)` for a single `reduce` call\n",
    "OutputFormat        |Reducer          |Writes `(key2,value2)` to HDFS file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Hadoop Streaming\n",
    "----------------\n",
    "\n",
    "<img src=\"images/hadoop-streaming.png\">\n",
    "\n",
    "Q: How does MRJob work under the hood?\n",
    "\n",
    "- Hadoop's MapReduce framework is written in Java.\n",
    "\n",
    "- The most direct way to write MapReduce jobs is in Java.\n",
    "\n",
    "- Hadoop also supports a Streaming API.\n",
    "\n",
    "- The Hadoop Streaming API lets you use any language to write Mappers\n",
    "  and Reducers.\n",
    "\n",
    "- It sends the data to the Streaming Mappers and Reducers of *standard\n",
    "  input* and *standard output*.\n",
    "\n",
    "\n",
    "Streaming Pros and Cons\n",
    "-----------------------\n",
    "\n",
    "<details><summary>\n",
    "Q: What are the pros and cons of Hadoop Streaming?\n",
    "</summary>\n",
    "Pros:\n",
    "<br>1. You can program in any language. E.g. you can use Perl, Python, or Ruby.\n",
    "<br>2. You can leverage libraries that you already have.\n",
    "<br>3. You can shorten development time.\n",
    "<br>Cons:\n",
    "<br>1. Hadoops spins up a separate interpreter process on every mapper\n",
    "and reducer, which uses up CPUs and memory.\n",
    "<br>2. The performance is not as good as using Java directly.\n",
    "<br>3. Binary types are not directly supported.\n",
    "<br>4. You can only write custom mappers and reducers---you cannot\n",
    "customize partitioners, input formats, and other parts of the\n",
    "MapReduce pipeline in other languages. For these you have to use\n",
    "Java or a JVM language.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Advanced MapReduce Applications\n",
    "----\n",
    "\n",
    "---\n",
    "What is a Combiner?\n",
    "----\n",
    "\n",
    "A ‘Combiner’ is a mini reducer that performs the local reduce task.\n",
    "\n",
    "It receives the input from the mapper on a particular node and sends the output to the reducer.\n",
    "\n",
    "Combiners help in enhancing the efficiency of MapReduce by reducing the quantum of data that is required to be sent to the reducers.\n",
    "\n",
    "![](http://4.bp.blogspot.com/-JK9zFB5WSl0/UpczHCmCQyI/AAAAAAAADw0/WwVgqWbRvgc/s1600/combiner.png)\n",
    "The goal is to find the minimum of the k-v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combiner Tips and Tricks\n",
    "-----\n",
    "\n",
    "- The *combiner* (if specified) is the reducer that the mapper uses to reduce the data locally.\n",
    "\n",
    "- What is the advantage of a combiner? \n",
    "> It reduces the disk footprint for the map output. Also it saves network bandwidth.\n",
    "\n",
    "- A reducer can only be used as a combiner if it is commutative and associative.\n",
    "\n",
    "![](https://californiaonlinehighschool.files.wordpress.com/2012/09/commutative_associative_properties.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transactions By State Using Combiner\n",
    "------------------------------------\n",
    "\n",
    "Q: How many transactions were there for each state?\n",
    "\n",
    "- Create the `SaleCountFast.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCountFast.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCountFast.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class SaleCountFast(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        yield (state, 1)\n",
    "    \n",
    "    def combiner(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "    \n",
    "    def reducer(self, state, counts): \n",
    "        yield state, sum(counts)\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleCountFast.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/step-0-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/step-0-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/step-0-reducer_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCountFast.brian.20160202.232925.413893\n"
     ]
    }
   ],
   "source": [
    "!python SaleCountFast.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t3\r\n",
      "\"OR\"\t1\r\n",
      "\"WA\"\t2\r\n"
     ]
    }
   ],
   "source": [
    "# Check the output.\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Can we use the reduce function as a combiner if we are calculating\n",
    "the total number of sales transactions per state?\n",
    "</summary>\n",
    "Yes.\n",
    "</details>\n",
    "\n",
    "<details><summary>\n",
    "Q: Can we use the reduce function as a combiner if we are calculating\n",
    "the average transaction revenue per state? </summary>\n",
    "1. No we cannot.\n",
    "<br>\n",
    "2. This is because average is non-associative.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Using Map-Only Job To Clean Data\n",
    "--------------------------------\n",
    "\n",
    "Q: Write an ETL application that extracts all the `CA` sales records.\n",
    "\n",
    "- This only requires transforming records, without consolidating them.\n",
    "\n",
    "- Any time we don't have to consolidate records we can use a *Map Only* job.\n",
    "\n",
    "- Create the `SaleExtract.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleExtract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleExtract.py\n",
    "\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class SaleExtract(MRJob):\n",
    "\n",
    "    def mapper_extract(self, _, line):\n",
    "        if line.startswith('#'): return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        if state != 'CA': return\n",
    "        yield (state, line)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_extract)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleExtract.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798/step-0-mapper_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleExtract.brian.20160202.232926.067798\n"
     ]
    }
   ],
   "source": [
    "!python SaleExtract.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"CA\"\t\"102    11/15/2014     203     CA     321        200.00\"\r\n",
      "\"CA\"\t\"106    11/19/2014     202     CA     331        330.00\"\r\n",
      "\"CA\"\t\"105    11/19/2014     202     CA     321        200.00\"\r\n"
     ]
    }
   ],
   "source": [
    "# Check the output:\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map-Only Applications\n",
    "---------------------\n",
    "\n",
    "Here are some other applications of map-only jobs.\n",
    "\n",
    "- Web-crawler that finds out how many jobs are on Craigslist for a\n",
    "  particular keyword.\n",
    "\n",
    "- Application that maps property addresses to property back-taxes by\n",
    "  scraping county databases.\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Do map-only applications shuffle and sort the data?\n",
    "</summary>\n",
    "1. No they do not shuffle and sort the data.\n",
    "<br>\n",
    "2. Map-only jobs immediately output the data after it is transformed\n",
    "   by map.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "Counters\n",
    "--------\n",
    "\n",
    "Counters are a special type of Map only job.\n",
    "\n",
    "For example, count how many transactions there were in California and Washington.\n",
    "\n",
    "- One way to solve this problem is to use a MapReduce application we\n",
    "  did before.\n",
    "\n",
    "- However, if we have a fixed number of categories we want to count we\n",
    "  can use counters.\n",
    "\n",
    "- If we use counters we no longer need a reduce phase, and can use a\n",
    "  map-only job.\n",
    "  \n",
    "- MapReduce has a limit of 120 counters.\n",
    "\n",
    "- So this cannot be used to count frequencies for an unknown number of\n",
    "  categories.\n",
    "\n",
    "- Create the `SaleCount1.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCount1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount1.py\n",
    "\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "\n",
    "class SaleCount1(MRJob):\n",
    "\n",
    "    def mapper_count(self, _, line):\n",
    "        if line.startswith('#'): return\n",
    "        fields = line.split()\n",
    "        state = fields[3]\n",
    "        if state == 'CA':\n",
    "            self.increment_counter('State', 'CA', 1)\n",
    "        if state == 'WA':\n",
    "            self.increment_counter('State', 'WA', 1)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__': \n",
    "    SaleCount1.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\r\n",
      "no configs found; falling back on auto-configuration\r\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738\r\n",
      "\r\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\r\n",
      "\r\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738/step-0-mapper_part-00000\r\n",
      "Counters from step 1:\r\n",
      "  State:\r\n",
      "    CA: 3\r\n",
      "    WA: 2\r\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738/step-0-mapper_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738/output/part-00000\r\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738/output\r\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount1.brian.20160202.232926.664738\r\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount1.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# There should not be any output. The counter values were printed when the job was executed.\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counter Notes\n",
    "-------------\n",
    "\n",
    "- Counters can be incremented in both the map and the reduce phase.\n",
    "\n",
    "- Counter values from all the machines participating in a MapReduce\n",
    "  job are aggregated to compute job-wide value.\n",
    "\n",
    "- Counter values are printed out when the job completes and are also\n",
    "  accessible on the Hadoop Web UI that stores job history.\n",
    "\n",
    "- Counters are have a group name and a counter name.\n",
    "\n",
    "- Group names help organize counters.\n",
    "\n",
    "- Here is how we increment a counter:\n",
    "  `self.increment_counter(group_name, counter_name, 1)`\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: SalesStrategy Inc employs 100,000 part-time sales partners to sell\n",
    "their products. The salespeople get monthly bonuses based on the\n",
    "number of transactions they ring up. Should SalesStrategy use counters\n",
    "to calculate these bonuses? Why or why not?\n",
    "</summary>\n",
    "1. Instead of counters they should use a regular MapReduce counting\n",
    "   application.\n",
    "<br>\n",
    "2. Counters are only appropriate if the number of categories is fixed\n",
    "   and is about 100.\n",
    "<br>\n",
    "3. While the Hadoop admin can configure the system to support more\n",
    "   counters than 120, this increases intra-cluster network traffic,\n",
    "   and is not recommended.\n",
    "</details>\n",
    "\n",
    "Map-Only Job Observations\n",
    "-------------------------\n",
    "\n",
    "- Map-only jobs are the multi-machine equivalent of the\n",
    "  multi-threading and multi-processing exercises we did earlier.\n",
    "\n",
    "- Like our multi-threading and multi-processing applications, map-only\n",
    "  jobs break up a larger problem into smaller chunks and then work on\n",
    "  a particular chunk.\n",
    "\n",
    "- Any time we have a problem where we don't need to reconcile or\n",
    "  consolidate records we should use map-only jobs.\n",
    "\n",
    "- Map-only jobs are much faster than regular MapReduce jobs.\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: Why are map-only jobs faster than regular MapReduce jobs?\n",
    "</summary>\n",
    "1. The map phase is perfectly parallelizable.\n",
    "<br>\n",
    "2. Map-only jobs don't have a shuffle-and-sort or reduce phase, which\n",
    "   tend to be the bottleneck for regular MapReduce jobs.\n",
    "</details>\n",
    "\n",
    "Chaining Jobs Together\n",
    "----------------------\n",
    "\n",
    "Q: Find word frequencies and sort the result by frequency. \n",
    "\n",
    "- This requires running two MapReduce jobs.\n",
    "\n",
    "- The first job will calculate word frequencies.\n",
    "\n",
    "- The second job will sort them.\n",
    "\n",
    "- This can be accomplished in MRJob by chaining multiple jobs together\n",
    "  as steps.\n",
    "\n",
    "- Create `MostUsedWords.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostUsedWords.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostUsedWords.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "class MostUsedWords(MRJob):\n",
    "\n",
    "    def mapper_get_words(self, _, line):\n",
    "        for word in WORD_RE.findall(line):\n",
    "            yield (word.lower(), 1)\n",
    "\n",
    "    def reducer_count_words(self, word, counts):\n",
    "        count_sum = '%03d'%sum(counts) \n",
    "        yield (count_sum, word)\n",
    "\n",
    "    def reducer_sort(self, count, words):\n",
    "        for word in words:\n",
    "            yield (word, count)\n",
    "\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_get_words,\n",
    "                   reducer=self.reducer_count_words),\n",
    "            MRStep(reducer=self.reducer_sort)\n",
    "        ]\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    MostUsedWords.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-0-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-0-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-1-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-1-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/step-1-reducer_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/MostUsedWords.brian.20160202.232927.186574\n"
     ]
    }
   ],
   "source": [
    "!python MostUsedWords.py input.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"again\"\t\"001\"\r\n",
      "\"second\"\t\"001\"\r\n",
      "\"third\"\t\"001\"\r\n",
      "\"world\"\t\"001\"\r\n",
      "\"hello\"\t\"002\"\r\n",
      "\"is\"\t\"002\"\r\n",
      "\"line\"\t\"002\"\r\n",
      "\"the\"\t\"002\"\r\n",
      "\"this\"\t\"002\"\r\n"
     ]
    }
   ],
   "source": [
    "# Check the output:\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Curse of the Last Reducer\n",
    "---\n",
    "\n",
    "These toy examples have had approximately uniform distributions.\n",
    "\n",
    "__Danger__: The real world is not uniform. Power-law distribution is very common.\n",
    "\n",
    "One poor set of Reducers will be overloaded. You'll be waiting on them to finish.\n",
    "\n",
    "This is the “curse of the last reducer”\n",
    "\n",
    "Very common in NLP and Graph Processing (for example, popular people on Facebook or Twitter) \n",
    "\n",
    "---\n",
    "Check for understanding\n",
    "---\n",
    "<details><summary>\n",
    "If you are running word count on the internet, what is the frequency distribution of words?\n",
    "</summary>\n",
    "Zipf's law\n",
    "<br>\n",
    "![](https://blogemis.files.wordpress.com/2015/09/graph-zipf.png)\n",
    "</details>\n",
    "\n",
    "[Read more here](http://www.slideserve.com/ura/counting-triangles-and-the-curse-of-the-last-reducer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "MapReduce Streaming API\n",
    "-----------------------\n",
    "\n",
    "- Why are we left-padding the amount with zeros? \n",
    "\n",
    "- MRJob is a wrapper around the MapReduce Streaming API.\n",
    "\n",
    "- The MapReduce Streaming API converts all intermediate types to strings for comparison.\n",
    "\n",
    "- So `123` will be smaller than `59` because it starts with `1` which\n",
    "  is less than `5`.\n",
    "  \n",
    "- To get around this in MRJob if we want our data to sort numerically\n",
    "  we have to left-pad the numbers with zeros.\n",
    "\n",
    "Check for understanding\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "Q: How can we find out which state had the highest sales total revenue?\n",
    "</summary>\n",
    "1. We can chain together two jobs.\n",
    "<br>\n",
    "2. The first one calculates revenue per state.\n",
    "<br>\n",
    "3. The second sorts the result of the first step by revenue.\n",
    "</details>\n",
    "\n",
    "Sorting Sales Data\n",
    "------------------\n",
    "\n",
    "Q: Find the total sales per state and then sort by sales to find the\n",
    "state with the highest sales total.\n",
    "\n",
    "- We can use a multi-step MRJob to do this.\n",
    "\n",
    "- Sort sales data using two steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting SaleCount.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile SaleCount.py\n",
    "from mrjob.job  import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import numpy as np\n",
    "\n",
    "class SaleCount(MRJob):\n",
    "   \n",
    "    def mapper1(self, _, line):\n",
    "        if line.startswith('#'):\n",
    "            return\n",
    "        fields = line.split()\n",
    "        amount = float(fields[5])\n",
    "        state = fields[3]\n",
    "        yield (state, amount)\n",
    "\n",
    "    def reducer1(self, state, amounts):\n",
    "        amount = '%07.2f'%sum(amounts) \n",
    "        yield (state, amount)\n",
    "    \n",
    "    def mapper2(self, state, amount):\n",
    "        yield (amount, state)\n",
    "\n",
    "    def reducer2(self, amount, states):\n",
    "        for state in states: \n",
    "            yield (state, amount)\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper1, reducer=self.reducer1),\n",
    "            MRStep(mapper=self.mapper2, reducer=self.reducer2)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    SaleCount.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Run it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no configs found; falling back on auto-configuration\n",
      "no configs found; falling back on auto-configuration\n",
      "creating tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006\n",
      "\n",
      "PLEASE NOTE: Starting in mrjob v0.5.0, protocols will be strict by default. It's recommended you run your job with --strict-protocols or set up mrjob.conf as described at https://pythonhosted.org/mrjob/whats-new.html#ready-for-strict-protocols\n",
      "\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-0-mapper_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-0-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-0-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-0-reducer_part-00000\n",
      "Counters from step 1:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-1-mapper_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-1-mapper-sorted\n",
      "> sort /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-1-mapper_part-00000\n",
      "writing to /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-1-reducer_part-00000\n",
      "Counters from step 2:\n",
      "  (no counters found)\n",
      "Moving /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/step-1-reducer_part-00000 -> /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/output/part-00000\n",
      "Streaming final output from /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006/output\n",
      "removing tmp directory /var/folders/ld/yffmln1s7z1cr9qmqgt9hkcw0000gn/T/SaleCount.brian.20160202.232928.301006\n"
     ]
    }
   ],
   "source": [
    "!python SaleCount.py sales.txt > output.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"OR\"\t\"0450.00\"\r\n",
      "\"CA\"\t\"0730.00\"\r\n",
      "\"WA\"\t\"1050.00\"\r\n"
     ]
    }
   ],
   "source": [
    "# Check the output:\n",
    "!cat output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Locality\n",
    "-------------\n",
    "\n",
    "Q: What is *data locality*?\n",
    "\n",
    "- Data locality is the secret sauce in HDFS and MapReduce. \n",
    "\n",
    "- Data locality means MapReduce runs mappers on locally machines with HDFS blocks.\n",
    "\n",
    "- When these machine are busy mappers may come up on other machines.\n",
    "\n",
    "\n",
    "Data Locality\n",
    "-------------\n",
    "\n",
    "![](https://cvw.cac.cornell.edu/MapReduce/images/hdfs_mr.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which machines run mappers and which run reducers?\n",
    "-----\n",
    "\n",
    "- The JobTracker tries to run the mappers on the machines where the\n",
    "  blocks of input data are located.\n",
    "\n",
    "- This is called data locality--ideally, the mapper does not need to\n",
    "  pull data across the network.\n",
    "\n",
    "- The reducers are assigned randomly to machines which have memory and\n",
    "  CPUs currently available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Building on top of MapReduce: Extensions on a faulty abstraction\n",
    "----\n",
    "\n",
    "![](http://cdn.meme.am/instances/61329997.jpg)\n",
    "\n",
    "Hive\n",
    "----\n",
    "\n",
    "- Hive was developed at Facebook.\n",
    "\n",
    "- It translates SQL to generate MapReduce code.\n",
    "\n",
    "- Its dialect of SQL is called HiveQL.\n",
    "\n",
    "- Data scientists can use SQL instead of MapReduce to process data.\n",
    "\n",
    "Pig\n",
    "---\n",
    "\n",
    "- Pig was developed at Yahoo.\n",
    "\n",
    "- It solves the same problem as Hive.\n",
    "\n",
    "- Pig uses a custom scripting language called PigLatin instead of SQL.\n",
    "\n",
    "- PigLatin resembles scripting languages like Python and Perl.\n",
    "\n",
    "- Pig is frequently used for processing unstructured or badly formed\n",
    "  data.\n",
    "\n",
    "Machine Learning\n",
    "---\n",
    "\n",
    "__DO NOT EVEN TRY!!!__\n",
    "\n",
    "![](http://www.roflcat.com/images/cats/All_Is_Lost.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Summary\n",
    "----\n",
    "\n",
    "- MapReduce is a distrubted batch processing framework\n",
    "- High throughput 😀and __not__: real-time / interactive / low latency 😦\n",
    "- All you get is functions on k-v pairs\n",
    "- 3 stages:\n",
    "    1. Map: Takes input and returns a key-value pairs.\n",
    "    2. Shuffle: Aligns the key-value pairs\n",
    "    3. Reduce: Aggregate function from key-value to key-value.\n",
    "- Plan your jobs at a (relatively) low-level to optimize compute resources\n",
    "- Avoid graph processing and machine learning in MapReduce (actually avoid MR altogether)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Extra\n",
    "----\n",
    "\n",
    "Pipes\n",
    "-----\n",
    "\n",
    "Q: What options do I have if I want to write my MapReduce code in\n",
    "C++?\n",
    "\n",
    "- For C++ there is a special interface that Hadoop provides called\n",
    "  *Pipes*.\n",
    "\n",
    "- Pipes is similar to streaming but instead of standard input and\n",
    "  output it uses sockets for communication.\n",
    "\n",
    "- Pipes is primarily used to leverage legacy code written in C++ or in\n",
    "  situations where a computation needs to use C++'s smaller memory\n",
    "  footprint for speed or scalability."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
