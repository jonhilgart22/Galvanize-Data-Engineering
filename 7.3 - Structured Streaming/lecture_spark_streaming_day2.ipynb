{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Spark Streaming\n",
    "====\n",
    "\n",
    "![](images/streaming_joke.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "By the end of this session, you should be able to:\n",
    "----\n",
    "\n",
    "- Describe how window and duration size impacts processing\n",
    "- Join streaming data\n",
    "- Handle fault tolerance with checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization - Windowing Operations With Inverse\n",
    "---------------------------------\n",
    "\n",
    "How can I avoid the overhead of adding or averaging over the same\n",
    "values in a window?\n",
    "\n",
    "```python\n",
    "windows_word_counts = pair_ds.reduceByKeyAndWindow(\n",
    "    func=lambda x, y: x + y,\n",
    "    invFunc=lambda x, y: x - y, \n",
    "    windowDuration=30,\n",
    "    slideDuration=10)\n",
    "```\n",
    "\n",
    "- Creates window of length `windowDuration` (30 seconds)\n",
    "\n",
    "- Moves window every `slideDuration` (10 seconds)\n",
    "\n",
    "- Merges incoming values using `func`\n",
    "\n",
    "- Eliminates outgoing values using `invFunc`\n",
    "\n",
    "- `windowDuration` and `slideDuration` are in seconds\n",
    "\n",
    "- These must be multiples of the `batchDuration` of the DStream\n",
    "\n",
    "- This requires that *checkpointing* is enabled on the StreamingContext.\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream.png\">\n",
    "\n",
    "<img src=\"images/streaming-windowed-stream-with-inv.png\">\n",
    "\n",
    "Streaming Durations\n",
    "-------------------\n",
    "\n",
    "What are the different durations in a DStream and which one should\n",
    "I use?\n",
    "\n",
    "Type               |Meaning\n",
    "----               |-------\n",
    "Batch Duration     |How many seconds until next incoming RDD\n",
    "Slide Duration     |How many seconds until next window RDD\n",
    "Window Duration    |How many seconds to include in window RDD\n",
    "\n",
    "![](http://4.bp.blogspot.com/-_HgluMg7Aa0/VHlgpJ99-0I/AAAAAAAABJc/adgprRDim-E/s1600/p1.png)\n",
    "\n",
    "Duration Impact\n",
    "---------------\n",
    "\n",
    "What is the impact of increasing these durations?\n",
    "\n",
    "Type                 |Increase                                   |Effect \n",
    "----                 |--------                                   |------ \n",
    "Batch Duration       |Larger but less frequent incoming RDDs     |Less Processing \n",
    "Slide Duration       |Less frequent window RDDs                  |Less Processing\n",
    "Window Duration      |Larger window RDDs                         |More Processing\n",
    "\n",
    "[Source](http://horicky.blogspot.com/2014/11/spark-streaming.html)\n",
    "\n",
    "Duration Summary\n",
    "----------------\n",
    "\n",
    "- Batch and window duration control RDD size\n",
    "\n",
    "- Batch and slide duration control RDD frequency\n",
    "\n",
    "- Larger RDDs have more context and produce better insights.\n",
    "\n",
    "- Larger RDDs might require more processing.\n",
    "\n",
    "- Bundling frequent small RDDs into infrequent larger ones can reduce processing.\n",
    "\n",
    "State DStreams\n",
    "--------------\n",
    "\n",
    "How can I aggregate a value over the lifetime of a streaming\n",
    "application?\n",
    "\n",
    "- You can do this with the `updateStateByKey` transform.\n",
    "\n",
    "```python\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "runningCounts = pairs.updateStateByKey(updateFunction)\n",
    "```\n",
    "\n",
    "- This takes a DStream made up of key-value RDDs\n",
    "\n",
    "- For each incoming RDD for each key it aggregates the values with the\n",
    "  previous values seen for that key.\n",
    "\n",
    "- Like the windowing transformations, this requires that checkpointing\n",
    "  be enabled on the StreamingContext.\n",
    "\n",
    "Testing Streaming Apps Using TextFileStream\n",
    "-------------------------------------------\n",
    "\n",
    "The QueueStream does not work with windowing operations or any\n",
    "other operations that require checkpointing. How can code that uses\n",
    "`updateStateByKey` be tested? \n",
    "\n",
    "- We can use TextFileStream instead.\n",
    "- Lets define a function `xrange_write` which we will use for the following examples.\n",
    "- This will write numbers 0, 1, 2, ... to directory `input`.\n",
    "- It will write 5 numbers per second, one per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting text_file_util.py\n"
     ]
    }
   ],
   "source": [
    "%%file text_file_util.py\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import uuid\n",
    "\n",
    "from distutils import dir_util \n",
    "\n",
    "# Every batch_duration write a file with batch_size numbers, forever.\n",
    "# Start at 0 and keep incrementing. (For testing.)\n",
    "\n",
    "def xrange_write(\n",
    "        batch_size = 5,\n",
    "        batch_dir = 'input',\n",
    "        batch_duration = 1):\n",
    "    dir_util.mkpath('./input')\n",
    "    \n",
    "    # Repeat forever\n",
    "    for i in itertools.count():\n",
    "        # Generate data\n",
    "        min = batch_size * i \n",
    "        max = batch_size * (i + 1)\n",
    "        batch_data = xrange(min,max)\n",
    "      \n",
    "        # Write to the file\n",
    "        unique_file_name = str(uuid.uuid4())\n",
    "        file_path = batch_dir + '/' + unique_file_name\n",
    "        with open(file_path,'w') as batch_file: \n",
    "            for element in batch_data:\n",
    "                line = str(element) + \"\\n\"\n",
    "                batch_file.write(line)\n",
    "    \n",
    "        # Give streaming app time to catch up\n",
    "        time.sleep(batch_duration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting Events\n",
    "---------------\n",
    "\n",
    "How can I count a certain type of event in incoming data?\n",
    "\n",
    "- You can use state DStreams.\n",
    "\n",
    "- This code takes a mod by 10 of the incoming numbers.\n",
    "\n",
    "- Then it counts how many times each number between 0 and 9 is seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_count.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_count.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "# add new values with previous running count to get new count\n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "ssc.checkpoint('ckpt')\n",
    "\n",
    "ds = ssc.textFileStream('input') \\\n",
    "    .map(lambda x: int(x) % 10) \\\n",
    "    .map(lambda x: (x,1)) \\\n",
    "    .updateStateByKey(updateFunction)\n",
    "\n",
    "ds.pprint()\n",
    "ds.count().pprint()\n",
    "\n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "\n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asimjalis/d/spark-1.6.0-bin-hadoop2.6/bin/spark-submit test_count.py\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#$SPARK_HOME/bin/spark-submit test_count.py\n",
    "echo $SPARK_HOME/bin/spark-submit test_count.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`.\n",
    "\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "How can you calculate a running average using a state DStream?\n",
    "</summary>\n",
    "1. In the above example, for the RDD key-value pair, replace `value`\n",
    "with `(sum,count)`. <br>\n",
    "2. In `updateStateByKey` add both to `sum` and `count`.<br>\n",
    "3. Use `map` to calculate `sum/count` which is the average.<br>\n",
    "</details>\n",
    "\n",
    "<!--\n",
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "How can you calculate a running standard deviation using a state DStream?\n",
    "</summary>\n",
    "1. See https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Online_algorithm<br>  <-- Wrong!\n",
    "</details>\n",
    "-->\n",
    "\n",
    "\n",
    "\n",
    "Join \n",
    "----\n",
    "\n",
    "How can I detect if an incoming credit card transaction is from a\n",
    "canceled card?\n",
    "\n",
    "- You can join DStreams against a batch RDD.\n",
    "- Store the historical data in the batch RDD.\n",
    "- Join it with the incoming DStream RDDs to determine next action.\n",
    "- Note: You must get the batch RDD using the `ssc.SparkContext`.\n",
    "\n",
    "```python\n",
    "dataset = ... # some RDD\n",
    "windowedStream = stream.window(20)\n",
    "joinedStream = windowedStream.transform(lambda rdd: rdd.join(dataset))\n",
    "```\n",
    "\n",
    "Detecting Bad Customers\n",
    "-----------------------\n",
    "Create a streaming app that can join the incoming orders with our\n",
    "previous knowledge of whether this customer is good or bad.\n",
    "\n",
    "- Create the streaming app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting test_join.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_join.py\n",
    "# Import modules.\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import time\n",
    "\n",
    "# Create the StreamingContext.\n",
    "\n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext(SparkContext(), batchDuration=1)\n",
    "\n",
    "\n",
    "# For testing create prepopulated QueueStream of streaming customer orders. \n",
    "\n",
    "print 'Initializing queue of customer transactions'\n",
    "transaction_rdd_queue = []\n",
    "for i in xrange(5): \n",
    "    transactions = [(customer_id, None) for customer_id in xrange(10)]\n",
    "    transaction_rdd = ssc.sparkContext.parallelize(transactions)\n",
    "    transaction_rdd_queue.append(transaction_rdd)\n",
    "pprint(transaction_rdd_queue)\n",
    "\n",
    "# Batch RDD of whether customers are good or bad. \n",
    "\n",
    "print 'Initializing bad customer rdd from batch sources'\n",
    "# (customer_id, is_good_customer)\n",
    "customers = [\n",
    "        (0,True),\n",
    "        (1,False),\n",
    "        (2,True),\n",
    "        (3,False),\n",
    "        (4,True),\n",
    "        (5,False),\n",
    "        (6,True),\n",
    "        (7,False),\n",
    "        (8,True),\n",
    "        (9,False) ]\n",
    "customer_rdd = ssc.sparkContext.parallelize(customers)\n",
    "\n",
    "# Join the streaming RDD and batch RDDs to filter out bad customers.\n",
    "print 'Creating queue stream'\n",
    "ds = ssc\\\n",
    "    .queueStream(transaction_rdd_queue)\\\n",
    "    .transform(lambda rdd: rdd.join(customer_rdd))\\\n",
    "    .filter(lambda (customer_id, (customer_data, is_good_customer)): is_good_customer)\n",
    "\n",
    "ds.pprint()\n",
    "\n",
    "ssc.start()\n",
    "time.sleep(6)\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ssc\n",
      "Initializing queue of customer transactions\n",
      "[ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[1] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[2] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[3] at parallelize at PythonRDD.scala:423,\n",
      " ParallelCollectionRDD[4] at parallelize at PythonRDD.scala:423]\n",
      "Initializing bad customer rdd from batch sources\n",
      "Creating queue stream\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:23\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(4, (None, True))\n",
      "(2, (None, True))\n",
      "(6, (None, True))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:24\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(4, (None, True))\n",
      "(2, (None, True))\n",
      "(6, (None, True))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:25\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(4, (None, True))\n",
      "(2, (None, True))\n",
      "(6, (None, True))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:26\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(4, (None, True))\n",
      "(2, (None, True))\n",
      "(6, (None, True))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:27\n",
      "-------------------------------------------\n",
      "(0, (None, True))\n",
      "(8, (None, True))\n",
      "(4, (None, True))\n",
      "(2, (None, True))\n",
      "(6, (None, True))\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2016-06-23 06:58:28\n",
      "-------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 0) / 4]\r",
      "[Stage 0:>                                                          (0 + 2) / 4]\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "# $SPARK_HOME/bin/spark-submit \n",
    "python test_join.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Challenge Question\n",
    "--------\n",
    "\n",
    "<details><summary>\n",
    "If you are joining with a large batch RDD how can you minimize the\n",
    "shuffling of the records?\n",
    "</summary>\n",
    "1. Use `partitionBy` on the incoming RDDs as well as on the batch\n",
    "RDD.<br>\n",
    "2. This will ensure that records are partitioned by their keys.<br>\n",
    "3. This can make a real difference in the performance of your Big Data\n",
    "streaming app.<br>\n",
    "</details>\n",
    "\n",
    "Cluster View\n",
    "------------\n",
    "\n",
    "<img src=\"images/streaming-daemons.png\">\n",
    "\n",
    "Checkpointing\n",
    "-------------\n",
    "\n",
    "How can I protect my streaming app against failure?\n",
    "\n",
    "- Streaming apps run for much longer than batch apps.\n",
    "\n",
    "- They can run for days and weeks.\n",
    "\n",
    "- So fault-tolerance is important for them.\n",
    "\n",
    "- To enable recovery from failure you must enable checkpointing.\n",
    "\n",
    "- If a checkpointed application crashes, you restart it and it\n",
    "  recovers the state of the RDDs when it crashed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing test_checkpointing.py\n"
     ]
    }
   ],
   "source": [
    "%%file test_checkpointing.py\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from text_file_util import xrange_write\n",
    "    \n",
    "from pprint import pprint\n",
    "    \n",
    "def updateFunction(newValues, runningCount):\n",
    "    if runningCount is None:\n",
    "       runningCount = 0\n",
    "    return sum(newValues, runningCount)  \n",
    "    \n",
    "checkpointDir = 'ckpt'\n",
    "    \n",
    "def functionToCreateContext():\n",
    "    ssc = StreamingContext(SparkContext(), batchDuration=2)\n",
    "    \n",
    "    # Add new values with previous running count to get new count\n",
    "    ds = ssc.textFileStream('input') \\\n",
    "        .map(lambda x: int(x) % 10) \\\n",
    "        .map(lambda x: (x,1)) \\\n",
    "        .updateStateByKey(updateFunction)\n",
    "    ds.pprint()\n",
    "    ds.count().pprint()\n",
    "    \n",
    "    # Set up checkpoint\n",
    "    ssc.checkpoint(checkpointDir)\n",
    "    return ssc\n",
    "    \n",
    "print 'Initializing ssc'\n",
    "ssc = StreamingContext.getOrCreate(\n",
    "    checkpointDir, functionToCreateContext)\n",
    "    \n",
    "print 'Starting ssc'\n",
    "ssc.start()\n",
    "    \n",
    "# Write data to textFileStream\n",
    "xrange_write()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets run this and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process is terminated.\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "$SPARK_HOME/bin/spark-submit test_checkpointing.py\n",
    "#echo $SPARK_HOME/bin/spark-submit test_checkpointing.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The program will run forever. To terminate hit `Ctrl-C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br> \n",
    "<br>\n",
    "\n",
    "----"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
